<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[ACEfish-Blog]]></title>
  <link href="https://acefish.github.io/atom.xml" rel="self"/>
  <link href="https://acefish.github.io/"/>
  <updated>2020-02-28T00:54:57+08:00</updated>
  <id>https://acefish.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im">MWeb</generator>

  
  <entry>
    <title type="html"><![CDATA[类簇]]></title>
    <link href="https://acefish.github.io/15828224173019.html"/>
    <updated>2020-02-28T00:53:37+08:00</updated>
    <id>https://acefish.github.io/15828224173019.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[事件响应链]]></title>
    <link href="https://acefish.github.io/15823466354492.html"/>
    <updated>2020-02-22T12:43:55+08:00</updated>
    <id>https://acefish.github.io/15823466354492.html</id>
    <content type="html"><![CDATA[
<p>参阅<a href="https://www.jianshu.com/p/b0884faae603">iOS事件处理，看我就够了~</a><br/>
参阅<a href="https://www.jianshu.com/p/c294d1bd963d">iOS触摸事件全家桶</a><br/>
参阅<a href="https://www.jianshu.com/p/df86508e2811">iOS 事件</a></p>

<h2 id="toc_0">触摸事件生成(触摸 &amp;&amp; 事件)</h2>

<h3 id="toc_1">UITouch</h3>

<ul>
<li>一个手指一次触摸屏幕，就对应生成一个UITouch对象。多个手指同时触摸屏幕，生成多个UITouch对象。</li>
<li>多个手指先后触摸，系统会根据触摸的位置判断是否更新同一个UITouch对象。若两个手指一前一后触摸同一个位置(即双击)，那么第一次触摸时生成一个UITouch对象，第二次触摸会更新这个UITouch对象，这是该UITouch对象的Tap Count属性值从1变成2，若两个手指一前一后触摸的位置不同，将会生成两个UITouch对象，两者之间没有联系</li>
<li>每个UITouch对象记录了触摸的一些信息，包括触摸时间、位置、阶段、所处的视图、窗口等信息。</li>
</ul>

<pre><code class="language-objectivec">// 触摸的各个阶段状态 
// 例如当手指移动时，会更新phase属性到UITouchPhaseMoved；
// 手指离屏后，更新到UITouchPhaseEnded
typedef NS_ENUM(NSInteger, UITouchPhase) {
    UITouchPhaseBegan,             // whenever a finger touches the surface.
    UITouchPhaseMoved,             // whenever a finger moves on the surface.
    UITouchPhaseStationary,        // whenever a finger is touching the surface but hasn&#39;t moved since the previous event.
    UITouchPhaseEnded,             // whenever a finger leaves the surface.
    UITouchPhaseCancelled,         // whenever a touch doesn&#39;t end but we need to stop tracking (e.g. putting device to face)
};
</code></pre>

<h3 id="toc_2">UIEvent</h3>

<p>触摸事件：<br/>
触摸的目的是生成触摸事件供响应者，一个触摸事件对应一个UIEvent对象，</p>

<p>UIEvent对象中的type属性标识了事件的类型</p>

<pre><code class="language-objectivec">typedef NS_ENUM(NSInteger, UIEventType) {
    UIEventTypeTouches, //touch事件
    UIEventTypeMotion, //运动事件
    UIEventTypeRemoteControl, //远程事件
    UIEventTypePresses NS_ENUM_AVAILABLE_IOS(9_0), //按压事件
};
</code></pre>

<p>UIEvent对象中包含了触发该对象的触摸对象集合</p>

<pre><code class="language-objectivec">@property(nonatomic, readonly, nullable) NSSet &lt;UITouch *&gt; *allTouches;
</code></pre>

<h3 id="toc_3">UIResponder</h3>

<p><code>UIResponder</code>是iOS中用于处理事件的API，不仅用于接收事件，还可以处理对应事件，如果当前响应者不能处理，就转发给其他响应者</p>

<p><code>UIResponder</code>类中包含了<code>touchesBegan</code>、<code>pressesBegan</code>、<code>motionBegan</code>、<code>remoteControlReceivedWithEvent</code>等方法，可以通过这些方法获取对应回调消息。</p>

<p>应用程序通过响应者来接收和处理事件，响应者可以是继承自<code>UIResponder</code>的任何子类，例如<code>UIView、UIViewController、UIApplication</code>等。当事件到来之际，系统会将事件传递给合适响应者，并成为<code>第一响应者</code>。如果<code>第一响应者</code>未处理事件，就会在响应者链中传递，其规则由<code>UIResponder</code>的<code>nextResponder</code>决定，可以通过重写该属性决定传递规则。当事件到来时，第一响应者没有接收消息，就顺着响应者链向后传递</p>

<h2 id="toc_4">Hit-Test 寻找第一响应者</h2>

<p>当点击屏幕上的view时，UIKit会打包一个<code>UIEvent</code>对象，将其分发给正在活跃的app，放在当前<code>Application</code>维护的事件队列中<br/>
此时app的<code>UIApplication</code>单例对象就会从事件队列中获取最新的事件，然后分发给能处理该事件的对象。此时app的任务就是获取能够响应该事件的UIView了 这就是通过<code>HitTest</code>的作用了</p>

<h3 id="toc_5">Hit-Test</h3>

<p><code>- (UIView *)hitTest:(CGPoint)point withEvent:(UIEvent *)event;</code> <br/>
<code>- (BOOL)pointInside:(CGPoint)point withEvent:(UIEvent *)event;</code>//判断点击区域是否在视图上，是则返回YES，不是返回NO</p>

<p><code>hitTest:withEvent:</code>方法内部会调用<code>pointInSide:withEvent:</code>方法，</p>

<p>具体流程：</p>

<ul>
<li><code>UIApplication</code>将事件传递给窗口对象<code>UIWindow</code>，优先询问后显示的窗口</li>
<li>如果<code>Window</code>不能响应事件，将事件传递给其它窗口；若窗口能响应事件，则<mark>从后向前</mark>(添加视图的逆序)询问窗口的子视图</li>
<li>类推，如果视图不响应事件，将其传递给同级的上一个视图，如果能响应，就遍历其子视图</li>
<li>如果其子视图都不能响应，则当前视图就是最合适的响应者</li>
</ul>

<p><code>hitTest:withEvent:</code>返回一个UIView对象,作为当前视图层次中的响应者。</p>

<p>其可能的实现为下：</p>

<pre><code class="language-objectivec">- (UIView *)hitTest:(CGPoint)point withEvent:(UIEvent *)event{
    //3种状态无法响应事件
     if (self.userInteractionEnabled == NO || self.hidden == YES ||  self.alpha &lt;= 0.01) return nil; 
    //触摸点若不在当前视图上则无法响应事件
    if ([self pointInside:point withEvent:event] == NO) return nil; 
    //从后往前遍历子视图数组 
    int count = (int)self.subviews.count; 
    for (int i = count - 1; i &gt;= 0; i--) 
    { 
        // 获取子视图
        UIView *childView = self.subviews[i]; 
        // 坐标系的转换,把触摸点在当前视图上坐标转换为在子视图上的坐标
        CGPoint childP = [self convertPoint:point toView:childView]; 
        //询问子视图层级中的最佳响应视图
        UIView *fitView = [childView hitTest:childP withEvent:event]; 
        if (fitView) 
        {
            //如果子视图中有更合适的就返回
            return fitView; 
        }
    } 
    //没有在子视图中找到更合适的响应视图，那么自身就是最合适的
    return self;
}
</code></pre>

<blockquote>
<p>注意：在log事件调用时，其视图hitTest和pointInSide方法会按照顺序调用两次</p>
</blockquote>

<h4 id="toc_6">视图无法响应事件的条件：</h4>

<ol>
<li>UIView的几种无法响应事件：
<ul>
<li>不允许交互：<code>userInteractionEnabled = NO</code></li>
<li>隐藏： <code>hidden = YES</code> 如果父视图隐藏 则子视图一会隐藏</li>
<li>透明度： alpha&lt;0.01  视图在0~0.01之间为透明  也会影响子视图透明度</li>
</ul></li>
<li>通过<code>pointInSide</code>的判断</li>
</ol>

<h2 id="toc_7">事件传递</h2>

<p>经过hit-test过程  UIApplication已经知道<code>第一响应者</code>，接下来</p>

<ul>
<li>将事件传递给第一响应者</li>
<li>将事件沿着响应链传递</li>
</ul>

<p><code>UIApplication</code>通过<code>sendEvent:</code>传递给事件所属window，然后window在通过<code>snedEvent:</code>将事件传递给<code>hit-tested View</code>即第一响应者</p>

<pre><code class="language-objectivec">//UIWindow
- (void)sendEvent:(UIEvent *)event;                    // called by UIApplication to dispatch events to views inside the window
</code></pre>

<p>通过在touchBegan的断点可以看到调用栈<br/>
<img src="media/15823466354492/15825512400627.jpg" alt="" style="width:625px;"/><br/>
其中window将UIEvent对象直接发送给对应hit-test View</p>

<blockquote>
<p>这是因为在hit-test过程时 将对应信息(对应的window和view)已经存入了UIEvent中的UITouch对象中</p>

<pre><code class="language-objectivec">//UITouch 的属性
@property(nullable,nonatomic,readonly,strong) UIWindow                        *window;
@property(nullable,nonatomic,readonly,strong) UIView                          *view;
</code></pre>
</blockquote>

<h3 id="toc_8">事件沿着响应链传递</h3>

<p>因为每个响应者一定是<code>UIRsponder</code>对象，其继承了4个响应触摸方法</p>

<pre><code class="language-objectivec">- (void)touchesBegan:(NSSet&lt;UITouch *&gt; *)touches withEvent:(nullable UIEvent *)event;
- (void)touchesMoved:(NSSet&lt;UITouch *&gt; *)touches withEvent:(nullable UIEvent *)event;
- (void)touchesEnded:(NSSet&lt;UITouch *&gt; *)touches withEvent:(nullable UIEvent *)event;
- (void)touchesCancelled:(NSSet&lt;UITouch *&gt; *)touches withEvent:(nullable UIEvent *)event;
- (void)touchesEstimatedPropertiesUpdated:(NSSet&lt;UITouch *&gt; *)touches API_AVAILABLE(ios(9.1));
</code></pre>

<p>默认不做任何处理 只是将事件沿着<code>响应链</code>(不是父类)传递<br/>
<code>第一响应者</code>首先收到触摸事件，具有对触摸事件处理权，可以自己对其进行处理(通过重写该方法)，也可以将事件沿着响应链传递给<code>下个响应者</code>，这个由<code>响应者</code>之间构成的<code>视图链</code>就称为<code>响应链</code>(因此响应的过程为自上向下的)</p>

<p>响应者对事件处理：<br/>
<mark>响应者对事件的拦截和传递都是通过<code>touchesBegan:withEvent:</code>方法控制的，即只有能执行到该方法,才能有可能获取其它3个触摸事件方法</mark></p>

<ul>
<li>不拦截,即不重写该方法，默认沿着响应链传递 </li>
<li>拦截， 重写<code>touchesBegan:withEvent:</code>，并不将事件向下传递</li>
<li>拦截，重写<code>touchesBegan:withEvent:</code>， 但再方法内部执行<code>[super touchesBegan:withEvent:]</code> 将方法向下传递</li>
</ul>

<h4 id="toc_9">传递规则</h4>

<p>每个<code>UIResponder</code>都有下一个<code>nextResponder</code>，获取响应链中的下个响应者</p>

<p>如下图所示，因为响应链上的前两个响应者未响应<code>began</code>方法，因此默认调用<code>[UIResponder touchesBegan:withEvent:]</code>方法，方法内部调用<code>[super touchesBegan:withEvent:]</code>则执行<code>fowardTouchMethod</code><br/>
<img src="media/15823466354492/15825536407581.jpg" alt="" style="width:830px;"/></p>

<ul>
<li>UIView<br/>
若视图是控制器的根视图，则其nextResponder为控制器对象；否则，其nextResponder为父视图。</li>
<li>UIViewController<br/>
若控制器的视图是window的根视图，则其nextResponder为窗口对象；若控制器是从别的控制器present出来的，则其nextResponder为presenting view controller。</li>
<li>UIWindow<br/>
nextResponder为UIApplication对象</li>
<li>UIApplication<br/>
若当前应用的app delegate是一个UIResponder对象，且不是UIView、UIViewController或app本身，则UIApplication的nextResponder为app delegate</li>
</ul>

<blockquote>
<p>可以通过<code>responder.nextResponder</code>获取响应链中的所有响应对象</p>
</blockquote>

<h2 id="toc_10">UIGestureRecognizer 手势</h2>

<p>手势分为 <code>离散型手势</code>和<code>持续型手势</code>.系统提供的点按手势<code>UITapGestureRecognizer</code>和轻扫手势<code>UISwipeGestureRecongnizer</code>，其余的均为持续型手势。</p>

<p>主要区别在于其状态变化的过程</p>

<pre><code class="language-text">//离散型

识别成功: Possible-&gt;Recongnized
识别失败: Possible-&gt;Failed

//持续型
完整识别: Possible-&gt;Began-&gt;[Changed]-&gt;Ended
不完整识别: Possible-&gt;Began-&gt;[Changed]-&gt;Cancel
</code></pre>

<p><img src="media/15823466354492/15827277237498.jpg" alt="" style="width:267px;"/><br/>
我们自定义了<code>FSTapView、</code><code>FSLongPressView、</code><code>FSTapGestureRecognizer、</code><code>FSLongPressGestureRecognizer、</code><code>FSButton</code>和<code>FSControl</code>并在类内部 重写4个事件处理方法，打印其方法执行，<mark>注意</mark>在重写方法最后调用super进行继续分发</p>

<blockquote>
<p>GestureRecognizer虽然不是继承自UIResponder但是内部也有事件处理方法 会在接下来介绍<br/>
如果在GestureRecognizer接收到TouchEvent事件而没有调用super继续分发 则会导致该手势识别失败</p>
</blockquote>

<h3 id="toc_11">离散手势</h3>

<p>点点击添加了自定义点按视图时<br/>
<img src="media/15823466354492/15827327818395.jpg" alt="" style="width:1116px;"/><br/>
<img src="media/15823466354492/15827327970288.jpg" alt="" style="width:1096px;"/></p>

<p><img src="media/15823466354492/15827326493908.jpg" alt="" style="width:1095px;"/></p>

<pre><code class="language-objectivec">2020-02-26 22:48:34.999518+0800 Hit-Testing_Learn[36770:469711] -[FSTapGestureRecognizer touchesBegan:withEvent:]
2020-02-26 22:48:35.000166+0800 Hit-Testing_Learn[36770:469711] -[TapView touchesBegan:withEvent:]
2020-02-26 22:48:35.001275+0800 Hit-Testing_Learn[36770:469711] -[FSTapGestureRecognizer touchesEnded:withEvent:]
2020-02-26 22:48:35.001735+0800 Hit-Testing_Learn[36770:469711] -[ViewController2 tapEvent:]
2020-02-26 22:48:35.001915+0800 Hit-Testing_Learn[36770:469711] -[TapView touchesCancelled:withEvent:]
</code></pre>

<p>可以看到</p>

<ul>
<li><code>UIWindow</code>将事件传递给第一响应者<code>TapView</code>前，将事件传递给了手势<code>FSTapGestureRecognizer</code></li>
<li>若手势识别成功(即收到了<code>touchesBegan和touchesEnded</code>事件),则取消第一响应者对事件响应<code>[TapView touchesCancelled:withEvent:]</code></li>
<li>若手势识别未成功，第一响应者接手对事件进行处理</li>
</ul>

<p>注意:</p>

<blockquote>
<p>对于<code>FSTapGestureRecognizer</code>必须对<code>touchesBegan和touchesEnded</code>方法调用super继续,否则会导致手势识别失败<br/>
对于手势识别器调用super并不是让其向下一个手势识别器传递事件，测试可得不论是否使用super 所有手势识别器都可以收到touch事件. 即window发送事件会将事件发送到所有的UItouch的数组中的所有手势识别器，发送并没有什么特定顺序，只有一个可以识别成功，并且当其识别成功其余的识别器则会全部失败，只有所有的手势识别器都识别失败，hit-testView 才可以接收触摸事件</p>

<p>//TODO:<br/>
对于在添加了点按手势的视图上move 如果不超过视图范围，则手势和视图都可以收到move事件，如果在视图范围里离开手指？ 如果在视图范围外离开手指？如果超过一段时间?</p>
</blockquote>

<p>因此，可以得出结论 <mark>UIGestureRecognizer比UIResponder具有更高的事件响应的优先级</mark></p>

<p>在苹果官方文档也有这么一段话：</p>

<pre><code class="language-text">window将事件传递给手势识别器，然后将其传递给hit-test的添加了手势的视图。假如手势识别器以多点触摸顺序分析触摸流，但是并没有识别出其手势，则View将接收所有touch。如果手势识别器识别出手势，则将取消视图的其余touch。手势识别的常规操作顺序遵循以下路径，路径由`cancelsTouchesInView``delaysTouchesBegan``delaysTouchesEnded`属性的默认值确定
</code></pre>

<p><img src="media/15823466354492/15827311973724.jpg" alt="" style="width:678px;"/><br/>
UIGestureRecognizer对事件的响应也是通过touch相关的4个方法来实现的，但是其并不是继承自UIResponder而是NSObject，而这4个方法声明在UIGestureRecognizerSubclass.h中。</p>

<p><code>UIWindow</code>之所以知道把时间传递给那些手势识别器，主要通过<code>UIEvent</code>的Touch中的<code>gestureRecognizers</code>数组，而这个数组也是hit-test View寻找第一响应者过程中填充的。</p>

<p>UIWindow会取出UIEvent里面的gestureRecognizers数组的手势识别器，将事件传递给各个手势识别器，如果有一个手势识别器识别了事件，其他的手势识别器就不会响应该事件。</p>

<p>因为<code>手势识别器</code>识别事件，需要一定时间，因此<code>tapView</code>会先调用<code>touchbegan</code>，而因为手势识别器识别成功，则<code>UIApplication</code>向tapView发送取消(cancel)事件响应</p>

<h3 id="toc_12">持续型手势</h3>

<pre><code class="language-objectivec">2020-02-27 00:10:36.580259+0800 Hit-Testing_Learn[38200:533365] -[FSLongPressGestureRecognizer touchesBegan:withEvent:]
2020-02-27 00:10:36.581306+0800 Hit-Testing_Learn[38200:533365] -[LongPressView touchesBegan:withEvent:]
2020-02-27 00:10:37.582065+0800 Hit-Testing_Learn[38200:533365] -[ViewController2 longPressEvent:]
2020-02-27 00:10:37.582259+0800 Hit-Testing_Learn[38200:533365] state --- 1
2020-02-27 00:10:37.582529+0800 Hit-Testing_Learn[38200:533365] -[LongPressView touchesCancelled:withEvent:]
2020-02-27 00:10:37.660837+0800 Hit-Testing_Learn[38200:533365] -[ViewController2 longPressEvent:]
2020-02-27 00:10:37.661041+0800 Hit-Testing_Learn[38200:533365] state --- 2
2020-02-27 00:10:37.661267+0800 Hit-Testing_Learn[38200:533365] -[ViewController2 longPressEvent:]
2020-02-27 00:10:37.661449+0800 Hit-Testing_Learn[38200:533365] state --- 2
2020-02-27 00:10:37.773423+0800 Hit-Testing_Learn[38200:533365] -[ViewController2 longPressEvent:]
2020-02-27 00:10:37.773574+0800 Hit-Testing_Learn[38200:533365] state --- 2
2020-02-27 00:10:37.949981+0800 Hit-Testing_Learn[38200:533365] -[ViewController2 longPressEvent:]
2020-02-27 00:10:37.950156+0800 Hit-Testing_Learn[38200:533365] state --- 2
2020-02-27 00:10:37.957969+0800 Hit-Testing_Learn[38200:533365] -[ViewController2 longPressEvent:]
2020-02-27 00:10:37.958148+0800 Hit-Testing_Learn[38200:533365] state --- 2
2020-02-27 00:10:37.967924+0800 Hit-Testing_Learn[38200:533365] -[FSLongPressGestureRecognizer touchesEnded:withEvent:]
2020-02-27 00:10:37.968218+0800 Hit-Testing_Learn[38200:533365] -[ViewController2 longPressEvent:]
2020-02-27 00:10:37.968356+0800 Hit-Testing_Learn[38200:533365] state --- 3
</code></pre>

<p><img src="media/15823466354492/15827335289202.jpg" alt="" style="width:1343px;"/></p>

<p><img src="media/15823466354492/15827335827964.jpg" alt="" style="width:1340px;"/></p>

<p><img src="media/15823466354492/15827336077836.jpg" alt="" style="width:1297px;"/></p>

<p>在堆栈可以看出，第一次调用时在runloop中通知监听的手势识别器的观察者，对其长按事件进行相应，此时state为<code>UIGestureRecognizerStateBegan</code></p>

<p>第二次、第三次调用是UIWindow 先将事件传递给UIEvent的gestureRecognizers数组里的手势识别器，然后长按手势识别器FJFLongPressGestureRecognizer识别成功进行回调,此时手势识别器的state为UIGestureRecognizerStateChanged 和 UIGestureRecognizerStateEnded。</p>

<h3 id="toc_13">总结</h3>

<p>当触摸发生或者触摸的状态发生变化时，UIWindow都会传递事件寻求响应。</p>

<p>-UIWindow先将触摸事件传递给响应链上绑定的手势识别器，再发送给触摸对象对应的第一响应者。</p>

<ul>
<li><p>手势识别器识别手势期间，若触摸对象的触摸状态发生变化，事件都是先发送给手势识别器，再发送给第一响应者。</p></li>
<li><p>手势识别器如果成功识别手势，则通知UIApplication取消第一响应者对于事件的响应，并停止向第一响应者发送事件。</p></li>
<li><p>如果手势识别器未能识别手势，而此时触摸并未结束，则停止向手势识别器发送事件，仅向第一响应者发送事件。</p></li>
<li><p>如果手势识别器未能识别手势，且此时触摸已经结束，则向第一响应者发送end状态的touch事件，以停止对事件的响应。</p></li>
</ul>

<h3 id="toc_14">拓展</h3>

<p>在<code>UIGestureRecognizer</code>中存在以下3个属性</p>

<pre><code class="language-objectivec">@property(nonatomic) BOOL cancelsTouchesInView;       // default is YES. causes touchesCancelled:withEvent: or pressesCancelled:withEvent: to be sent to the view for all touches or presses recognized as part of this gesture immediately before the action method is called.
@property(nonatomic) BOOL delaysTouchesBegan;         // default is NO.  causes all touch or press events to be delivered to the target view only after this gesture has failed recognition. set to YES to prevent views from processing any touches or presses that may be recognized as part of this gesture
@property(nonatomic) BOOL delaysTouchesEnded;         // default is YES.  仅在此手势识别失败后，才将touchesEnded或pressesEnded事件传递到目标视图。 这样可以确保如果识别出手势，可以取消手势的触摸或按动
</code></pre>

<ol>
<li><p>cancelsTouchesInView</p>
<p>默认为YES。表示当手势识别器成功识别了手势之后，会通知Application取消响应链对事件的响应，并不再传递事件给第一响应者。若设置成NO，表示手势识别成功后不取消响应链对事件的响应，事件依旧会传递给第一响应者</p></li>
<li><p>delaysTouchesBegan <br/>
默认为NO。默认情况下手势识别器在识别手势期间，当触摸状态发生改变时，Application都会将事件传递给手势识别器和第一响应者；若设置成YES，则表示手势识别器在识别手势期间，截断事件，即不会将事件发送给第一响应者。</p></li>
<li><p>delaysTouchesEnded<br/>
默认为YES。当手势识别失败时，若此时触摸已经结束，会延迟一小段时间（0.15s）再调用响应者的touchesEnded:withEvent:；若设置成NO，则在手势识别失败时会立即通知Application发送状态为end的touch事件给第一响应者以调用 touchesEnded:withEvent:结束事件响应</p></li>
</ol>

<h2 id="toc_15">UIControl</h2>

<p><code>UIControl</code>是系统提供的能够以<code>target-action</code>模式处理触摸事件的控件，iOS中UIButton、UISegmentedControl、UISwitch等都是其子类<br/>
因为 UIControl是UIView子类，因此本身也具有<code>UIResponder</code>身份</p>

<p><code>UIControl</code>作为控件类的基类，它是一个<code>抽象基类</code>，我们不能直接使用UIControl类来实例化控件，它只是为控件子类定义一些通用的接口，并提供一些基础实现，以在事件发生时，预处理这些消息并将它们发送到指定目标对象上。</p>

<h3 id="toc_16">Target-Action机制</h3>

<p>当事件将要发生时，事件会被发送到控件对象，然后控件对象触发target对象上的action行为，最终处理事件。<br/>
<code>UIControl</code>能够响应事件的控件，必须也需要事件交互符合条件才去响应，因此也会追踪事件发生的过程。和<code>UIResponder</code>以及<code>UIGestureRecongnizer</code>通过<code>touches</code>系列方法追踪，<code>UIControl</code>有独特的追踪方式:</p>

<pre><code class="language-objectivec">- (BOOL)beginTrackingWithTouch:(UITouch *)touch withEvent:(UIEvent *)event {
    NSLog(@&quot;%s&quot;,__func__);
    return YES;
}
- (BOOL)continueTrackingWithTouch:(UITouch *)touch withEvent:(nullable UIEvent *)event {
    NSLog(@&quot;%s&quot;,__func__);
    return YES;
}
- (void)endTrackingWithTouch:(nullable UITouch *)touch withEvent:(nullable UIEvent *)event {
    NSLog(@&quot;%s&quot;,__func__);
}// touch is sometimes nil if cancelTracking calls through to this.
- (void)cancelTrackingWithEvent:(nullable UIEvent *)event {
    NSLog(@&quot;%s&quot;,__func__);
}   // event may be nil if cancelled for non-event reasons, e.g. removed from window
</code></pre>

<p>这4个方法和<code>UIResponder</code>的方法几乎温和,只是<code>UIControl</code>只能接收单点触控，因此参数是单个<code>UITouch</code>对象。和UIResponder职能类似，也是为了跟踪触摸的开始、滑动、结束、取消。不过，<code>UIControl</code>本身也是<code>UIResponder</code>，因此同样有touches系列的4个方法，事实上，UIControl的<code>Tracking</code>系列方法是在touch系列方法内部调用的。因此，UIControl中<code>touches</code>系列方法的默认实现和UIResponder还是有区别的</p>

<pre><code class="language-objectivec">2020-02-28 00:20:48.544827+0800 Hit-Testing_Learn[65809:1215013] -[FSTapGestureRecognizer touchesBegan:withEvent:]
2020-02-28 00:20:48.545294+0800 Hit-Testing_Learn[65809:1215013] -[FSButton touchesBegan:withEvent:]
2020-02-28 00:20:48.545425+0800 Hit-Testing_Learn[65809:1215013] -[FSButton beginTrackingWithTouch:withEvent:]
2020-02-28 00:20:48.545971+0800 Hit-Testing_Learn[65809:1215013] -[FSTapGestureRecognizer touchesEnded:withEvent:]
2020-02-28 00:20:48.546185+0800 Hit-Testing_Learn[65809:1215013] -[FSButton touchesEnded:withEvent:]
2020-02-28 00:20:48.546290+0800 Hit-Testing_Learn[65809:1215013] -[FSButton endTrackingWithTouch:withEvent:]
2020-02-28 00:20:48.546447+0800 Hit-Testing_Learn[65809:1215013] -[ViewController2 btnClickEvent:]
</code></pre>

<p><img src="media/15823466354492/15828208315732.jpg" alt="" style="width:1107px;"/><br/>
<img src="media/15823466354492/15828208589479.jpg" alt="" style="width:1084px;"/></p>

<ul>
<li>可以看到，Window先将时间传递给手势识别器，再传递给第一响应者<code>FSButton</code></li>
<li>手势识别器和第一响应者分别调用touch相关方法对事件进行识别</li>
<li><mark>最终第一响应者对事件进行相应调用<code>sendAction:to:forEvent</code>将target和action发送给UIApplication,UIApplication通过<code>sendAction:to:from:forevent</code>向target发送action</mark></li>
</ul>

<p>再来看自定义继承UIControl的<code>FSControl</code></p>

<pre><code class="language-objectivec">2020-02-28 00:32:26.644690+0800 Hit-Testing_Learn[66062:1228563] -[FSTapGestureRecognizer touchesBegan:withEvent:]
2020-02-28 00:32:26.645519+0800 Hit-Testing_Learn[66062:1228563] -[FSControl touchesBegan:withEvent:]
2020-02-28 00:32:26.645689+0800 Hit-Testing_Learn[66062:1228563] -[FSControl beginTrackingWithTouch:withEvent:]
2020-02-28 00:32:26.646929+0800 Hit-Testing_Learn[66062:1228563] -[FSTapGestureRecognizer touchesEnded:withEvent:]
2020-02-28 00:32:26.647320+0800 Hit-Testing_Learn[66062:1228563] -[ViewController2 tapEvent:]
2020-02-28 00:32:26.647492+0800 Hit-Testing_Learn[66062:1228563] -[FSControl touchesCancelled:withEvent:]
2020-02-28 00:32:26.647634+0800 Hit-Testing_Learn[66062:1228563] -[FSControl cancelTrackingWithEvent:]
</code></pre>

<p>此时结果和之前的<code>FSButton</code>优先级比手势识别器高截然相反，优先级竟然更低了</p>

<p>经过验证:<br/>
系统提供的有默认action操作的<code>UIControl</code>，例如<code>UIbutton、UISwitch</code>等的单击，<code>UIControl</code>的响应优先级比手势识别器高,而对于自定义的<code>UIControl</code>，响应的优先级比手势低<br/>
至于为什么会这样，没找到具体原因，但测试的结果，推测系统应该是依据UITouch的touchIdentifier来进行区别处理</p>

<h4 id="toc_17">target-action管理</h4>

<pre><code class="language-objectivec">
// 添加
- (void)addTarget:(id)target action:(SEL)action forControlEvents:(UIControlEvents)controlEvents
 // 删除
- (void)removeTarget:(id)target action:(SEL)action forControlEvents:(UIControlEvents)controlEvents

// 获取控件对象所有相关的target对象
// get info about target &amp; actions. this makes it possible to enumerate all target/actions by checking for each event kind
// 集合中可能包含NSNull 表明至少有一个nil目标对象
@property(nonatomic,readonly) NSSet *allTargets; 

- (nullable NSArray&lt;NSString *&gt; *)actionsForTarget:(nullable id)target forControlEvent:(UIControlEvents)controlEvent;    // single event. returns NSArray of NSString selector names. returns nil if none
</code></pre>

<p><code>UIControl</code>内部实际上有一个可变数组来保存<code>Target-Action</code>，数组中每个元素都是<code>UIControlTargetAction</code>对象。该类是私有类</p>

<pre><code class="language-objectivec">@interface UIControlTargetAction : NSObject {
    SEL _action;
    BOOL _cancelled;
    unsigned int _eventMask;// 事件类型,比如:UIControlEventTouchUpInside
    id _target;
}
</code></pre>

<h2 id="toc_18">总结  事件完整响应链</h2>

<ol>
<li>系统通过<code>IOKit.framework</code>来处理硬件操作，其中屏幕处理也通过IOKit完成(IOKit可能是注册监听了屏幕输出的端口)<br/>
当用户操作屏幕，IOKit收到屏幕操作，会将这次操作封装为IOHIDEvent对象。通过mach port(IPC进程间通信)将事件转发给SpringBoard来处理</li>
<li>SpringBoard是iOS系统的桌面程序。SpringBoard收到mach port发过来的事件，唤醒main runloop来处理</li>
<li>main runloop将事件交给source1处理，<code>source1</code>会调用<code>__IOHIDEventSystemClientQueueCallback()</code>函数<br/>
该函数会在内部判断是否有程序在前台显示，如果有通过<code>mach port</code>将<code>IOHIDEvent</code>事件转发给这个程序<br/>
如果前台没有程序在显示，则表明SpringBoard的桌面程序在前台显示，也就是用户在桌面进行了操作。<code>__IOHIDEventSystemClientQueueCallback()</code>函数会将事件交给<code>source0</code>处理，<code>source0</code>会调用<code>__UIApplicationHandleEventQueue()</code>函数，函数内部会做具体的处理操作</li>
<li>例如用户点击了某个应用程序的icon，会将这个程序启动。<br/>
应用程序接收到<code>SpringBoard</code>传来的消息，会唤醒<code>main runloop</code>并将这个消息交给source1处理，source1调用<code>__IOHIDEventSystemClientQueueCallback()</code>函数，在函数内部会将事件交给<code>source0</code>处理，并调用source0的<code>__UIApplicationHandleEventQueue()</code>函数。在__UIApplicationHandleEventQueue()函数中，会将传递过来的<code>IOHIDEvent</code>转换为<code>UIEvent</code>对象</li>
<li>在函数内部，将事件放入UIApplication的事件队列，等到处理该事件时，将该事件出队列，UIApplication将事件传递给窗口对象(UIWindow)，如果存在多个窗口，则从后往前询问最上层显示的窗口</li>
<li>窗口UIWindow通过hitTest和pointInside操作，判断是否可以响应事件，如果窗口UIWindow不能响应事件，则将事件传递给其他窗口；若窗口能响应事件，则从后往前询问窗口的子视图。</li>
<li>以此类推，如果当前视图不能响应事件，则将事件传递给同级的上一个子视图；如果能响应，就从后往前遍历当前视图的子视图</li>
<li>如果当前视图的子视图都不能响应事件，则当前视图就是第一响应者</li>
<li>找到第一响应者，事件的传递的响应链也就确定的</li>
<li>如果第一响应者非UIControl子类且响应链上也没有绑定手势识别器<code>UIGestureRecognizer</code>;</li>
<li>那么由于第一响应者具有处理事件的最高优先级，因此UIApplication会先将事件传递给它供其处理。首先，<code>UIApplication</code>将事件通过<code>sendEvent:</code>传递给事件所属的window，window同样通过<code>sendEvent:</code>再将事件传递给<code>hit-tested view</code>，即第一响应者,第一响应者具有对事件的完全处理权，默认对事件不进行处理，传递给下一个响应者(nextResponder)；如果响应链上的对象一直没有处理该事件，则最后会交给<code>UIApplication</code>，如果UIApplication实现代理，会交给<code>UIApplicationDelegate</code>，如果<code>UIApplicationDelegate</code>没处理，则该事件会被丢弃</li>
<li>如果第一响应者非UIControl子类但响应链上也绑定了手势识别器<code>UIGestureRecognizer</code></li>
<li>UIWindow会将事件先发送给响应链上绑定的手势识别器<code>UIGestureRecognizer</code>，再发送给第一响应者，如果手势识别器能成功识别事件，UIApplication默认会向第一响应者发送cancel响应事件的命令;如果手势识别器未能识别手势，而此时触摸并未结束，则停止向手势识别器发送事件，仅向第一响应者发送事件。如果手势识别器未能识别手势，且此时触摸已经结束，则向第一响应者发送end状态的touch事件，以停止对事件的响应</li>
<li>如果第一响应者是自定义的UIControl的子类同时响应链上也绑定了手势识别器<code>UIGestureRecognizer</code>;这种情况跟第一响应者非UIControl子类但响应链上也绑定了手势识别器<code>UIGestureRecognizer</code>处理逻辑一样</li>
<li>如果第一响应者是UIControl的子类且是系统类(UIButton、UISwitch)同时响应链上也绑定了手势识别器<code>UIGestureRecognizer</code></li>
<li>UIWindow会将事件先发送给响应链上绑定的手势识别器<code>UIGestureRecognizer</code>，再发送给第一响应者，如果第一响应者能响应事件，UIControl调用调用<code>sendAction:to:forEvent:</code>将target、action以及event对象发送给UIApplication，UIApplication对象再通过<code>sendAction:to:from:forEvent:</code>向target发送action</li>
</ol>

<h2 id="toc_19">示例：</h2>

<h3 id="toc_20">超出子视图范围</h3>

<p><img src="media/15823466354492/15824747327255.jpg" alt="" style="width:219px;"/><br/>
此时 当点击btn不在BView时，并不起作用，因为点击的点在btn的父视图Bview外 则事件无法传递到btn，如果想要点击外部区域也起作用</p>

<pre><code class="language-objectivec">//在BView重写
//处理超出区域点击无效的问题
- (UIView *)hitTest:(CGPoint)point withEvent:(UIEvent *)event {
    
    if (self.isHidden == NO) {
        // 转换坐标
        CGPoint newPoint = [self convertPoint:point toView:self.btn];
        // 判断点击的点是否在按钮区域内
        if ( [self.btn pointInside:newPoint withEvent:event]) {
            //返回按钮
            return self.btn;
        }else{
            return [super hitTest:point withEvent:event];
        }
    }
    else {
        return [super hitTest:point withEvent:event];
    }
}
</code></pre>

<h3 id="toc_21">想要扩大视图的点击范围</h3>

<p>假如想要扩大按钮的点击范围</p>

<pre><code class="language-objectivec">//TabBar
- (BOOL)pointInside:(CGPoint)point withEvent:(UIEvent *)event {
    //将触摸点坐标转换到在CircleButton上的坐标
    CGPoint pointTemp = [self convertPoint:point toView:self.indicateView];
    //若触摸点在CricleButton上则返回YES
    if ([self.indicateView pointInside:pointTemp withEvent:event]) {
        return YES;
    }
    //否则返回默认的操作
    return [super pointInside:point withEvent:event];
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Objective-C二进制瘦身]]></title>
    <link href="https://acefish.github.io/15822611342755.html"/>
    <updated>2020-02-21T12:58:54+08:00</updated>
    <id>https://acefish.github.io/15822611342755.html</id>
    <content type="html"><![CDATA[
<p>参阅<a href="https://www.jianshu.com/p/e3cf048c67aa">Objective-C二进制瘦身</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[]]></title>
    <link href="https://acefish.github.io/15819548506333.html"/>
    <updated>2020-02-17T23:54:10+08:00</updated>
    <id>https://acefish.github.io/15819548506333.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[优化OpenGL ES app]]></title>
    <link href="https://acefish.github.io/15819546549863.html"/>
    <updated>2020-02-17T23:50:54+08:00</updated>
    <id>https://acefish.github.io/15819546549863.html</id>
    <content type="html"><![CDATA[
<p><a href="https://developer.apple.com/library/archive/documentation/3DDrawing/Conceptual/OpenGLES_ProgrammingGuide/Performance/Performance.html">Tuning Your OpenGL ES App</a></p>

<h2 id="toc_0">使用Xcode和Instruments调试和分析您的应用</h2>

<h2 id="toc_1">一般性能优化建议</h2>

<h2 id="toc_2">高效地使用基于图块的延迟渲染</h2>

<h2 id="toc_3">最小化绘图命令的数量</h2>

<h2 id="toc_4">最小化OpenGL ES内存使用</h2>

<h2 id="toc_5">注意Core Animation合成性能</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[构造Audio Unit应用]]></title>
    <link href="https://acefish.github.io/15776919276552.html"/>
    <updated>2019-12-30T15:45:27+08:00</updated>
    <id>https://acefish.github.io/15776919276552.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">选择设计模式</h2>

<p>有6种设计模式可以用于iOS应用audio unit。首先选择一种最接近你的app处理音频的模式。<br/>
其共同特性：</p>

<ul>
<li>仅有一个I/O unit</li>
<li>在整个<code>audio processing graph</code>中使用单一音频流格式，尽管格式可能会有变化</li>
<li>要求在特定位置设置流的格式或者部分流的格式</li>
</ul>

<blockquote>
<p>正确设置流格式对于建立音频数据流至关重要</p>
</blockquote>

<h3 id="toc_1">I/O传递</h3>

<p>将传入的音频直接发送到输出硬件，不需要处理audio data。<br/>
<img src="media/15776919276552/15776937507092.jpg" alt="" style="width:468px;"/></p>

<p>音频硬件强制<code>Remote I/O unit</code>输入元素的朝外一侧的流格式，而可以在朝内侧指定期望的格式。audio unit执行需要的格式转换。为避免不必要的采样率转换，请定义采样率为硬件设备采样率</p>

<p>利用两个remote I/O元素间的<code>audio unit connection</code>，不需要设置output元素的input scope格式。<code>connection</code>会为传播在输入元素指定的格式</p>

<p>在output 元素的外侧采用硬件流格式，其会自动执行需要的格式转换</p>

<blockquote>
<p>input 元素默认是禁用的，因此在使用时 需要确保启用</p>

<p>使用这个模式 无需配置任何音频数据缓冲区</p>
</blockquote>

<h3 id="toc_2">没有渲染回调的I/O</h3>

<p><img src="media/15776919276552/15776943788896.jpg" alt="" style="width:590px;"/></p>

<p>在<code>remote I/O</code>元素间 添加一个或多个audio unit。<br/>
这种模式 仍然没有回调函数，就说明了，无法直接操作音频</p>

<blockquote>
<p>无论何时，使用<code>I/O unit</code>外的任何unit，都必须要指定<code>kAudioUnitProperty_MaximumFramesPerSlice</code>属性</p>

<p>要设置<code>Multichannel Mixer unit</code>就必须在混音器的输出上设置流格式的采样率</p>

<p>使用这个模式 类似直通模式 也无需配置任何音频数据缓冲区</p>
</blockquote>

<h3 id="toc_3">具有渲染回调的I/O</h3>

<p><img src="media/15776919276552/15776953809911.jpg" alt="" style="width:737px;"/></p>

<p>这种模式，在<code>Remote I/O</code>的输入和输出元素间放置渲染回调函数，可以在音频达到输出硬件前对其进行操作。简单可以使用render回调调整音量，复杂的话，可以通过<code>Accelerate framework</code><a href="https://developer.apple.com/documentation/accelerate?language=objc">Accelerate</a>中的<code>Fourier transforms</code>和卷积函数对声音做处理</p>

<p>将回调函数附加到输出元素的输入范围，回调函数通过调用<code>Remote I/O</code>的input元素的渲染回调函数获取新的音频数据</p>

<blockquote>
<p>此时，当使用渲染回调函数建立从一个audio unit到另一个unit的路径，回调函数将代替<code>audio unit connection</code></p>
</blockquote>

<h3 id="toc_4">仅有渲染回调函数的 output</h3>

<p>常为音乐游戏和合成器选择这种模式，此时app需要生成声音并最大程度的响应。</p>

<p>简单来说，这个模式将渲染回调函数直接连接到远程I/O unit的输出元素的输入范围<br/>
<img src="media/15776919276552/15776968166755.jpg" alt="" style="width:603px;"/></p>

<p>可以使用相同模式构造更复杂结构的app<br/>
<img src="media/15776919276552/15776970706094.jpg" alt="" style="width:644px;"/></p>

<blockquote>
<p>iPod EQ要求您在输入和输出上都设置完整的流格式</p>

<p>多通道混频器只需要在其输出上设置正确的采样率即可</p>

<p><code>Multichannel Mixer unit inputs</code>通常需要单独考虑每个音频单元的流格式</p>
</blockquote>

<h3 id="toc_5">其它设计模式</h3>

<p>还有两种其他设计模式：</p>

<ul>
<li><p>录制或分析音频，创建具有渲染回调功能的仅input  app<br/>
一般，更好的选择是使用<code>AudioQueue</code>对象(<code>AudioQueueRef</code>),此时将会有更大灵活性，因为其渲染回调并不在实时线程上</p></li>
<li><p>执行离线音频处理，使用<code>Generic Output unit</code>，此unit并未连接到音频硬件。当使用它向应用程序发送音频时，取决于您app来调用其render方法</p></li>
</ul>

<h2 id="toc_6">构建你的app</h2>

<p>构建步骤：</p>

<ol>
<li>配置<code>audio session</code></li>
<li>指定 <code>audio unit</code></li>
<li>创建一个<code>audio processing graph</code> 然后获取<code>audio unit</code></li>
<li>配置<code>audio unit</code></li>
<li>连接<code>audio unit node</code></li>
<li>提供用户界面</li>
<li>初始化 并启动音频处理图</li>
</ol>

<h3 id="toc_7">配置audio session</h3>

<p>与配置所有iOS音频相同，第一步就是配置<code>audio session</code>。</p>

<p>首先指定在app中采用的采样率：<code>self.graphSampleRate = 44100.0; // Hertz</code><br/>
接着使用<code>audio session</code>对象 请求系统将您的首选采样率用作设备硬件采样率，来避免app间的采样率转换，这样可以最大化CPU性能和声音质量，并最大程度地减少电池消耗。</p>

<pre><code class="language-objectivec">NSError *audioSessionError = nil;
//获取对应用程序的单例音频回话
AVAudioSession *mySession = [AVAudioSession sharedInstance];     // 1
//请求硬件的采样率，根据设备上其它音频活动，系统可能不会通过请求
[mySession setPreferredHardwareSampleRate: graphSampleRate       // 2
                                    error: &amp;audioSessionError];
//请求音频回话类别，指定&quot;录制和播放&quot;支持音频输入和输出
[mySession setCategory: AVAudioSessionCategoryPlayAndRecord      // 3
                                    error: &amp;audioSessionError];
//请求激活音频回话
[mySession setActive: YES                                        // 4
               error: &amp;audioSessionError];
//激活后 根据系统提供采样率 更新自己采样率
self.graphSampleRate = [mySession currentHardwareSampleRate];    // 5
</code></pre>

<p>还有另外一项硬件特性：音频硬件I/O缓冲区持续时间。在44.1kHZ采样率下，默认持续时间为23ms，相当于1024个采样的切片大小。如果I/O延迟在应用中很重要，可以请求更短的持续时间，大约0.005ms(相当于256个样本)</p>

<pre><code class="language-objectivec">self.ioBufferDuration = 0.005;
[mySession setPreferredIOBufferDuration: ioBufferDuration
                                  error: &amp;audioSessionError];
</code></pre>

<p>参阅<a href="https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">Audio Session Programming Guide</a></p>

<h3 id="toc_8">指定所需音频单元</h3>

<p>使用<code>AudioComponentDescription</code>结构指定所需的每个<code>audio unit</code>.</p>

<h3 id="toc_9">构建Audio Processing Graph</h3>

<p>这一步将创建前面所说的设计模式框架。</p>

<pre><code class="language-objectivec">//如何对包含Remote I/O单元和Multichannel Mixer unit的图形执行这些步骤
AUGraph processingGraph;
NewAUGraph (&amp;processingGraph);

AUNode ioNode;
AUNode mixerNode;
//
AUGraphAddNode (processingGraph, &amp;ioUnitDesc, &amp;ioNode);
AUGraphAddNode (processingGraph, &amp;mixerDesc, &amp;mixerNode);
</code></pre>

<p>此时该graph已实例化，并拥有您将在应用程序中使用的nodes。<br/>
然后open graph并初始化audio unit</p>

<pre><code class="language-objectivec">AUGraphOpen (processingGraph);
</code></pre>

<p>然后获取audio unit实例的引用</p>

<pre><code class="language-objectivec">AudioUnit ioUnit;
AudioUnit mixerUnit;
 
AUGraphNodeInfo (processingGraph, ioNode, NULL, &amp;ioUnit);
AUGraphNodeInfo (processingGraph, mixerNode, NULL, &amp;mixerUnit);
</code></pre>

<p>获取了unit对象的引用，就可以配置或者互连这些audio unit</p>

<h3 id="toc_10">配置audio unit</h3>

<p>每个unit都有自己的配置，参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html#//apple_ref/doc/uid/TP40009492-CH17-SW1">Using Specific Audio Units</a></p>

<p>默认 <code>Remote I/O</code>已启用输出并禁用输入。</p>

<p>除了<code>Remote I/O</code>和<code>Voice-processing I/O</code>单元外，所有iOS音频单元都需要配置<code>kAudioUnitProperty_MaximumFramesPerSlice</code>属性。此属性可确保audio unit准备好响应渲染调用而产生足够数量的音频数据帧。参阅<a href="https://developer.apple.com/documentation/audiounit/audio_unit_properties?language=objc">Audio Unit Properties</a></p>

<p>所有的<code>audio unit</code>都需要在输入或者输出上定义其频流格式。</p>

<h3 id="toc_11">编写和渲染回调函数</h3>

<p>有关示例，请在iOS库中查看各种音频单元示例项目包括<code>MixerHost、aurioTouch、SynthHost</code></p>

<pre><code class="language-objectivec">AURenderCallbackStruct callbackStruct;
callbackStruct.inputProc        = &amp;renderCallback;
callbackStruct.inputProcRefCon  = soundStructArray;
 
AudioUnitSetProperty (
    myIOUnit,
    kAudioUnitProperty_SetRenderCallback,
    kAudioUnitScope_Input,
    0,                 // output element
    &amp;callbackStruct,
    sizeof (callbackStruct)
);

//使用AUGraph 以线程安全的方式
AURenderCallbackStruct callbackStruct;
callbackStruct.inputProc        = &amp;renderCallback;
callbackStruct.inputProcRefCon  = soundStructArray;
 
AUGraphSetNodeInputCallback (
    processingGraph,
    myIONode,
    0,                 // output element
    &amp;callbackStruct
);
// ... some time later
Boolean graphUpdated;
AUGraphUpdate (processingGraph, &amp;graphUpdated);

</code></pre>

<h3 id="toc_12">连接 Audio Unit Nodes</h3>

<p>大多数情况下，使用<code>AUdio processing Graph</code>中的API <code>AUGraphConnectNodeInput、AUGraphDisconnectNodeInput</code>函数在audio unit间建立和断开连接，这些函数是线程安全的，而且避免显式定义连接的编码开销</p>

<pre><code class="language-objectivec">AudioUnitElement mixerUnitOutputBus  = 0;
AudioUnitElement ioUnitOutputElement = 0;
 
AUGraphConnectNodeInput (
    processingGraph,
    mixerNode,           // source node
    mixerUnitOutputBus,  // source node bus
    iONode,              // destination node
    ioUnitOutputElement  // desinatation node element
);
</code></pre>

<p>不推荐：可以直接使用音频单元属性机制在音频单元之间建立和断开连接。为此，请结合使用<code>AudioUnitSetProperty</code>函数和<code>kAudioUnitProperty_MakeConnection</code>属性。此方法要求您为每个连接定义一个<code>AudioUnitConnection</code>结构，以用作其属性值</p>

<pre><code class="language-objectivec">AudioUnitElement mixerUnitOutputBus  = 0;
AudioUnitElement ioUnitOutputElement = 0;
 
AudioUnitConnection mixerOutToIoUnitIn;
mixerOutToIoUnitIn.sourceAudioUnit    = mixerUnitInstance;
mixerOutToIoUnitIn.sourceOutputNumber = mixerUnitOutputBus;
mixerOutToIoUnitIn.destInputNumber    = ioUnitOutputElement;
 
AudioUnitSetProperty (
    ioUnitInstance,                     // connection destination
    kAudioUnitProperty_MakeConnection,  // property key
    kAudioUnitScope_Input,              // destination scope
    ioUnitOutputElement,                // destination element
    &amp;mixerOutToIoUnitIn,                // connection definition
    sizeof (mixerOutToIoUnitIn)
);
</code></pre>

<h3 id="toc_13">提供用户界面</h3>

<p>参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW21">Use Parameters and UIKit to Give Users Control</a></p>

<h3 id="toc_14">初始化并且开始Audio Processing Graph</h3>

<p>开始音频流之前，必须通过调用<code>AUGraphInitialize</code>函数来初始化音频处理图。<br/>
这一步执行的步骤：</p>

<ul>
<li>通过自动为每个unit单独调用<code>AudioUnitInitialize</code>函数初始化所有audio unit。</li>
<li>验证图表的连接和音频数据流格式。</li>
<li>在音频单元连接之间传播流格式。</li>
</ul>

<pre><code class="language-objectivec">OSStatus result = AUGraphInitialize (processingGraph);
// Check for error. On successful initialization, start the graph...
AUGraphStart (processingGraph);
 
// Some time later
AUGraphStop (processingGraph)
</code></pre>

<blockquote>
<p>如果graph 初始化失败，可以使用<code>CAShow</code>函数，将<code>graph</code>的状态输出到Xcode控制台</p>
</blockquote>

<h2 id="toc_15">调试</h2>

<p>打印ASBD信息 查看问题</p>

<pre><code class="language-objectivec">- (void) printASBD: (AudioStreamBasicDescription) asbd {
 
    char formatIDString[5];
    UInt32 formatID = CFSwapInt32HostToBig (asbd.mFormatID);
    bcopy (&amp;formatID, formatIDString, 4);
    formatIDString[4] = &#39;\0&#39;;
 
    NSLog (@&quot;  Sample Rate:         %10.0f&quot;,  asbd.mSampleRate);
    NSLog (@&quot;  Format ID:           %10s&quot;,    formatIDString);
    NSLog (@&quot;  Format Flags:        %10X&quot;,    asbd.mFormatFlags);
    NSLog (@&quot;  Bytes per Packet:    %10d&quot;,    asbd.mBytesPerPacket);
    NSLog (@&quot;  Frames per Packet:   %10d&quot;,    asbd.mFramesPerPacket);
    NSLog (@&quot;  Bytes per Frame:     %10d&quot;,    asbd.mBytesPerFrame);
    NSLog (@&quot;  Channels per Frame:  %10d&quot;,    asbd.mChannelsPerFrame);
    NSLog (@&quot;  Bits per Channel:    %10d&quot;,    asbd.mBitsPerChannel);
}
</code></pre>

<h2 id="toc_16">Using Specific Audio Units</h2>

<p><a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/UsingSpecificAudioUnits/UsingSpecificAudioUnits.html">Using Specific Audio Units</a></p>

<h3 id="toc_17">Using I/O Units</h3>

<h4 id="toc_18">Remote I/O Unit</h4>

<p>参见示例代码项目<a href="https://developer.apple.com/library/archive/samplecode/aurioTouch/Introduction/Intro.html#//apple_ref/doc/uid/DTS40007770">aurioTouch</a></p>

<h3 id="toc_19">Using Mixer Units</h3>

<h4 id="toc_20">Multichannel Mixer Unit</h4>

<p>示例项目<a href="">Audio Mixer (MixerHost)</a></p>

<p>默认情况下，<code>kAudioUnitProperty_MaximumFramesPerSlice</code>属性设置为值1024，这在屏幕锁定和显示器休眠时是不够的。如果您的应用在锁定屏幕的情况下播放音频，则除非音频输入处于活动状态，否则您必须增加此属性的值。进行如下操作：</p>

<ul>
<li>如果音频输入处于活动状态，则无需为<code>kAudioUnitProperty_MaximumFramesPerSlice</code>属性设置值。</li>
<li>如果未激活音频输入，则将此属性设置为4096</li>
</ul>

<h3 id="toc_21">Using Effect Units</h3>

<p><a href="">Mixer iPodEQ AUGraph Test</a>示例代码项目</p>

<h2 id="toc_22">各种音频单元标识符</h2>

<p><img src="media/15776919276552/15777080977092.jpg" alt="" style="width:879px;"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Audio Unit 工作原理]]></title>
    <link href="https://acefish.github.io/15773512971221.html"/>
    <updated>2019-12-26T17:08:17+08:00</updated>
    <id>https://acefish.github.io/15773512971221.html</id>
    <content type="html"><![CDATA[
<p>参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/Introduction/Introduction.html#//apple_ref/doc/uid/TP40009492-CH1-SW1">Audio Unit Hosting Guide for iOS</a><br/>
<a href="https://developer.apple.com/documentation/audiounit?language=objc">AudioUnit framework</a></p>

<p><code>Audio Unit</code>用于向应用程序添加复杂的音频处理功能。还可以在宿主应用中创建或者修改audio的<code>Audio unit extension</code>。</p>

<p>iOS 提供了音频处理插件，支持音频混合、均衡、格式转换以及用于录制、播放、离线渲染、实时对话(例如VoiP)的实时输入和输出。可以在iOS应用程序中动态的加载和使用这些插件，这些托管的插件即为<code>audio unit</code></p>

<p>audio unit 通常在 <code>audio processing graph</code>上下文中工作。<br/>
<img src="media/15773512971221/15775974949411.jpg" alt="" style="width:500px;"/></p>

<p>因为audio unit 是iOS中最底层的音频编程层，因此其需要更深入的了解，因此 除非你需要实时播放同步声音、低延迟的I/O操作或者特定的unit功能呢，否则请先考虑使用<code>Media Player、AV Foundation、OpenAL或Audio Toolbox</code>这些高级结束，参阅<a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/MediaPlaybackGuide/Contents/Resources/en.lproj/Introduction/Introduction.html#//apple_ref/doc/uid/TP40016757">Media Playback Programming Guide</a></p>

<p>Audio unit的生命周期：</p>

<ol>
<li>在运行时，获取对可动态链接库的引用，库中定义了要使用的 audio unit</li>
<li>实例化<code>audio unit</code></li>
<li>根据类型要求配置音频单元，适应自己的应用意图</li>
<li>初始化audio unit 准备处理音频</li>
<li>开始音频流</li>
<li>控制audio unit</li>
<li>结束后，销毁unit</li>
</ol>

<p>构建托管音频单元的应用程序的步骤<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/ConstructingAudioUnitApps/ConstructingAudioUnitApps.html#//apple_ref/doc/uid/TP40009492-CH16-SW1">Constructing Audio Unit Apps</a>：</p>

<ol>
<li>配置应用程序的audio session确保应用程序可以在系统和设备硬件的上下文中正常工作</li>
<li>构造一个<code>audio processing graph(AUGraph)</code>.参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitHostingGuide_iOS/AudioUnitHostingFundamentals/AudioUnitHostingFundamentals.html#//apple_ref/doc/uid/TP40009492-CH3-SW11">Audio Unit Hosting Fundamentals</a></li>
<li>提供用于控制图的audio unit 的用户接口</li>
</ol>

<blockquote>
<p>可以下载iOS开发中心的示例程序<a href="">Audio Mixer(MixerHost)</a></p>
</blockquote>

<p>参考文档：<br/>
<a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/MediaPlaybackGuide/Contents/Resources/en.lproj/Introduction/Introduction.html#//apple_ref/doc/uid/TP40016757">Media Playback Programming Guide</a> 学习了解各种音频技术 来检查是否有更高级别技术可以满足您的音频要求</p>

<p><a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/WhatisCoreAudio/WhatisCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH3-SW11">A Little About Digital Audio and Linear PCM</a> 学习相关音频基础知识 概念</p>

<p><a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Reference/CoreAudioGlossary/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004453">Core Audio Glossary </a> 来查看是否有不懂的专业术语概念</p>

<h2 id="toc_0">Audio unit 基础知识</h2>

<p>iOS中的所有音频技术都是建立在<code>audio unit</code>之上的。<br/>
<img src="media/15773512971221/15776007315531.jpg" alt="" style="width:444px;"/></p>

<p>因此建议仅当需要以下条件时，可以使用unit，否则请使用高级的API：</p>

<ul>
<li>低延迟的同步音频I/O(例如 VoIP)</li>
<li>响应性的播放合成声音，例如 音乐游戏或合成乐器</li>
<li>使用特定音频单元功能，例如 回升消除、混音或音调均衡</li>
<li>使用链式处理体系结构，可以让音频处理模块组装到灵活的网中。这是iOS中唯一提供此功能的API</li>
</ul>

<h3 id="toc_1">iOS中的Audio unit</h3>

<p>iOS中提供了七个unit，按照目的分为4类</p>

<ol>
<li>Effect： iPod Equalizer</li>
<li>Mixing： 3D Minxer、Multichannel Mixer</li>
<li>I/O：Remote I/O、Voice-Processing I/O、Generic Output、</li>
<li>Format conversion：Formater conversion</li>
</ol>

<h5 id="toc_2">Effect Unit</h5>

<p>音效单元iPod 均衡器，其与内置的iPod应用程序使用的均衡器相同。此unit提供了一组预设的均衡曲线</p>

<h5 id="toc_3">Mixer Units</h5>

<p>iOS中有两个混音单元。<br/>
<code>3D Mixer</code>是构建OpenAL的基础(大多数需要3D mixer功能呢最好选择使用 Open AL)，提供了更适合应用程序的更高级的API。<br/>
<code>Multichannel Mixer unit</code>可以为任意数量的单声道或者立体声流提供混音，并带有立体声输出。可以关闭或者打开每个输入、设置输入增益、设置立体声声像位置。参见示例项目<code>Audio Mixer（MixerHost）</code></p>

<h5 id="toc_4">I/O Units</h5>

<p>iOS中有3个I/O单元</p>

<ol>
<li><code>Remote I/O</code>是最常用的，连接到输入输出音频硬件，可以低延迟的访问输入输出音频样本值，并用附带的音频转换器单元在硬件音频格式和应欧阳程序音频果然是间提供格式转换。参见项目<a href="https://developer.apple.com/library/archive/samplecode/aurioTouch/Introduction/Intro.html#//apple_ref/doc/uid/DTS40007770">aurioTouch</a></li>
<li><code>Voice-Processing I/O unit</code>通过添加用于<code>聊天应用或者VoIP</code>的回音消除功能来扩展Remote I/O，还提供了自动增益教正、语音处理质量调整和静音功能</li>
<li><code>Generic Output unit</code> 不连接到音频硬件，而是提供一种将处理链的输出发送到应用程序的机制。通常，试用期进行离线音频处理</li>
</ol>

<h5 id="toc_5">Format Converter Unit</h5>

<p>通常通过I/O单元间接使用</p>

<h3 id="toc_6">使用Audio unit两种API</h3>

<p>iOS有可直接使用audio unit的一个API，而另一个API用于处理<code>audio processing graph</code>。当在应用中使用audio unit可以同时使用两个API</p>

<ul>
<li>直接使用audio unit <a href="https://developer.apple.com/documentation/audiounit/audio_unit_component_services?language=objc">Audio Unit Component Services</a></li>
<li>创建和配置audio processing graph <a href="https://developer.apple.com/documentation/audiotoolbox/audio_unit_processing_graph_services">Audio Unit Processing Graph Services</a></li>
</ul>

<p>推荐使用<code>audio processing graph</code>API  使代码更加易于阅读，并且支持动态重新配置</p>

<h3 id="toc_7">指定和获取audio unit</h3>

<p>在运行时查找<code>audio unit</code>，请先在<code>audio component description data</code>结构中指定其类型、子类型和制造商。不论使用audio unit还是graph API都要执行此操作</p>

<pre><code class="language-objectivec">AudioComponentDescription ioUnitDescription;
 
ioUnitDescription.componentType          = kAudioUnitType_Output;
ioUnitDescription.componentSubType       = kAudioUnitSubType_RemoteIO;
ioUnitDescription.componentManufacturer  = kAudioUnitManufacturer_Apple;
ioUnitDescription.componentFlags         = 0;
ioUnitDescription.componentFlagsMask     = 0;
</code></pre>

<blockquote>
<p>所有的iOS音频单元都使用<code>kAudioUnitManufacturer_Apple</code></p>

<p>如果要创建通配符，将字段设置为0.例如为匹配所有的I/O单元，将<code>componentSubType</code>字段设置值为0</p>
</blockquote>

<p>有了description  就可以使用两个API之一 获取指定音频单元</p>

<pre><code class="language-objectivec">AudioComponent foundIoUnitReference = AudioComponentFindNext (
                                          NULL,
                                          &amp;ioUnitDescription
                                      );
AudioUnit ioUnitInstance;
AudioComponentInstanceNew (
    foundIoUnitReference,
    &amp;ioUnitInstance
);
</code></pre>

<blockquote>
<p>传递NULL 给第一个参数，表示使用系统定义的顺序来找到和描述匹配的第一个系统音频单元。如果在这个参数传递先前已经找到的音频单元，则会查找下一个匹配的audio unit，这常用于通过重复调用<code>AudioComponentFindNext</code>来获取所偶遇I/O单元引用<br/>
<code>AudioComponentFindNext</code>函数结果是对定义的audio unit的<code>dynamically-linkable library</code>的引用，将其传递给<code>AudioComponentInstanceNew</code> 实例化音频单元</p>
</blockquote>

<pre><code class="language-objectivec">// Declare and instantiate an audio processing graph
AUGraph processingGraph;
NewAUGraph (&amp;processingGraph);
 
// Add an audio unit node to the graph, then instantiate the audio unit
AUNode ioNode;
AUGraphAddNode (
    processingGraph,
    &amp;ioUnitDescription,
    &amp;ioNode
);
AUGraphOpen (processingGraph); // 间接执行音频单元的实例化 
// 获取对新实例化的I/O单元的引用
AudioUnit ioUnit;
AUGraphNodeInfo (
    processingGraph,
    ioNode,
    NULL,
    &amp;ioUnit
);
</code></pre>

<p>这种方式引入了<code>AUNode</code>，这是在graph中代表unit的不透明类型。</p>

<h3 id="toc_8">使用scope和element指定Audio Units各部分</h3>

<p><code>audio unit</code>的各部分按scope和element组织。当调用一个函数时配置和控制<code>audio unit</code>，指定scope和element来标识函数的特定目标<br/>
<img src="media/15773512971221/15776367807541.png" alt="" style="width:300px;"/></p>

<p><code>scope</code>是<code>audio unit</code>的程序化上下文。这些context是绝不会互相嵌套的，可以使用<code>Audio Unit Scopes</code>枚举常量来指定范围</p>

<p><code>element</code>是嵌套在audio unit scope内的程序化上下文。当element是输入输出范围一部分时，类似物理音频设备中的信号总线，因此有时称为总线。 <mark>可以通过0索引的整数值指定element</mark>如果设置应用于整个scope的属性或者参数，指定元素值为0</p>

<p>不同的audio unit可能有不同的架构。<code>global scope</code>适用于所有的audio unit，并且不与特定的特定音频流相关联，它只有一个element 0，而某些属性也只能应用于global scope(kAudioUnitProperty_MaximumFramesPerSlice)。</p>

<p>上图说明了一种音频单元的通用体系结构，其中输入和输出上的元素数量相同。但是，各种音频单元使用各种架构。例如，混音器单元可能有多个输入元素，但只有一个输出元素</p>

<p>输入和输出scope直接参与音频流在audio unit间的流动。可以使用属性或者参数整体应用于输入、输出scope，而其他的属性和参数则适用于特定scope内的element</p>

<h3 id="toc_9">使用属性配置audio unit</h3>

<p><code>audio unit</code>属性是用于配置音频单元的键值对。键为带助记符的枚举整数例如<code>kAudioUnitProperty_MaximumFramesPerSlice = 14</code>。值为指定的数据类型。</p>

<p>使用<code>AudioUnitSetProperty</code>函数设置属性，并指定scope和<code>element</code></p>

<pre><code class="language-objectivec">UInt32 busCount = 2;
 
OSStatus result = AudioUnitSetProperty (
    mixerUnit,
    kAudioUnitProperty_ElementCount,   // the property key
    kAudioUnitScope_Input,             // the scope to set the property on
    0,                                 // the element to set the property on
    &amp;busCount,                         // the property value
    sizeof (busCount)
);
</code></pre>

<p>audio unit开发中的一些属性：</p>

<ul>
<li><code>KAudioOutputUnitProperty_EnableIO</code>:用于启用或禁用I/O单元上的输入和输出。默认，启动输出，但禁用输入</li>
<li><code>KAudioUnitProperty_ElementCount</code>:配置<code>mixer unit</code>输入元素数量</li>
<li><code>KAudioUnitProperty_MaximumFramesPerSlice</code>：用于指定audio unit应准备音频数据的最大帧来响应render call。对于大多数音频单元，在大多数情况下，必须按照参考文档中的说明设置此属性。否则，屏幕锁定时，音频将停止。</li>
<li><code>KAudioUnitProperty_StreamFormat</code>:用于指定特定音频单元输入或输出总线的音频流数据格式。
参阅<a href="https://developer.apple.com/documentation/audiounit/audio_unit_properties?language=objc">Audio Unit Properties</a></li>
</ul>

<p>大对数的属性值仅在audio unit未初始化时才能设置，这类属性不能由用户更改。但是对于例如<code>iPod EQ unit</code>中的<code>kAudioUnitProperty_PresentPreset</code>和 <code>Voice-Processing I/O</code>单元中的<code>kAUVoiceIOProperty_MuteOutput</code>属性播放音频时是会被更改的</p>

<p>查找可用属性：</p>

<ul>
<li><code>AudioUnitGetPropertyInfo</code>：查找属性是否可用，如果是，将提供其值的数据大小以及是否可以更改</li>
<li><code>AudioUnitGetProperty、AudioUnitSetProperty</code>:用于获取或者设置属性值</li>
<li><code>AudioUnitAddPropertyListener、AudioUnitRemovePropertyListenerWithUserData</code>:安装或者删除用于监视属性值更改的回调函数</li>
</ul>

<h3 id="toc_10">使用参数和UIKit 给用户控制</h3>

<p><code>audio unit</code>参数数用户可控制的，在产生音频时可以实时更改，对正在执行的处理实时调整。与属性类似，参数也是键值对，键位枚举常量，但是值是相同类型<code>32位浮点数</code>。具体参数信息可以参阅<a href="https://developer.apple.com/documentation/audiounit/audio_unit_parameters?language=objc">Audio Unit Parameters</a></p>

<p>使用以下函数获取和设置参数值：</p>

<ul>
<li>AudioUnitGetParameter</li>
<li>AudioUnitSetParameter</li>
</ul>

<p>用户可以通过UI界面配置参数值，参见示例项目<code>MixerHost</code></p>

<h3 id="toc_11">I/O unit 基本特征</h3>

<p>I/O单元是每个audio unit应用都必须使用，在一个I/Ounit中包含两个元素</p>

<p><img src="media/15773512971221/15776750118116.jpg" alt="" style="width:500px;"/></p>

<p>我们在应用中常将这两个element看做独立实体。例如根据需要用属性启用或者禁用I/O每个元素</p>

<p><code>element 1</code>的输入范围直接连接到音频输入硬件，这是对开发不透明的，我们首次对音频数据的访问是在element 1的输出范围。同理，<code>element 0</code>的连接到音频输出硬件，这也是不透明的，我们只需要负责将音频传动到输入范围。</p>

<p><code>I/O unit</code>是唯一能够启动或者停止<code>audio processing graph</code>中音频流的audio unit。因此，<code>I/O unit</code>负责audio unit中的音频流</p>

<h3 id="toc_12">Audio Processing Graphs</h3>

<p><code>Audio Processing Graphs</code>就是不透明类型<code>AUGraph</code>，用于构造和管理audio unit的处理链。<br/>
<code>AUGraph</code>类型增加audio units的线程安全，使您可以动态的重新配置处理链。</p>

<p>在<code>AUGraph</code>中使用又一种不透明类型<code>AUNode</code>来表示图形上下文中的音频单元。此时，我们通常将node作为unit代理与其交互，而不直接与unit交互。</p>

<p>但是，一个图形时，必须通过音频单元API配置每个<code>audio unit</code>。audio unit node是不可配置的。</p>

<p>可以通过定义代表完整音频处理子图的Node，将<code>AUNode</code>作为复杂graph中的实例。此时，子图的末尾I/O必须是<code>Generic Output unit</code>(其不与硬件设备连接)</p>

<p>因此构建graph的任务：</p>

<ol>
<li>向图中添加子节点</li>
<li>直接配置节点代表的audio unit</li>
<li>将node互连</li>
</ol>

<h4 id="toc_13">一个Audio Processing Graph 有一个 I/O Unit</h4>

<p><code>Audio Processing Graph</code>都有一个I/O unit，其<code>I/O unit</code>可以是各种类型具体取决于应用需求。</p>

<p><code>Graph</code>通过<code>AUGraphStart</code>和<code>AUGraphStop</code>启动和停止音频流。这些函数又通过<code>AudioOutputUnitStart</code>和<code>AudioOutputUnitStop</code>将消息传递给I/O unit。因此，I/O unit将负责graph中的<code>audio stream</code></p>

<h4 id="toc_14">Audio Processing Graphs提供了线程安全</h4>

<h4 id="toc_15">Audio 在Graph 中 通过 pull 流动</h4>

<p>在<code>audio processing graph</code>中，在unit在需要更多数据时调用其provider。这种数据的请求流和音频流方向相反<br/>
<img src="media/15773512971221/15776864609106.jpg" alt="" style="width:462px;"/><br/>
对一组数据的每个请求称为render call或者随意的称为pull。render call请求的数据更加恰当的称为<code>audio sample frame</code>。<br/>
响应render call而提供的一组<code>audio sample frame</code>称为<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Reference/CoreAudioGlossary/Glossary/core_audio_glossary.html#//apple_ref/doc/uid/TP40004453-CH210-SW165">slice</a>。提供slice的函数称为<code>render call back</code>(渲染回调函数)。</p>

<p>整个工作流程</p>

<ol>
<li>调用AUGraphStart函数后，虚拟输出设备触发<code>Remote I/O</code>输出元素的渲染回调函数。这个函数请求一片处理过的音频数据帧</li>
<li><code>Remote I/O</code>的回调函数在输入缓冲区查找要处理的音频数据，如果有待处理的数据将使用它，否则它将调用应用程序中连接到其输入的任何内容的渲染回调。在上图中，I/O unit从effct unit中请求以片数据</li>
<li><code>effect unit</code>行为和I/O unit 相同，当需要数据时，使用了app的渲染回调函数</li>
<li>app的渲染回调函数是该请求的最终接收者，将请求的帧提供给效果器</li>
<li>效果单元处理app 回调函数提供的slice。然后将其处理的数据提供给I/O unit</li>
<li><code>Remote I/O</code>处理slice，然后将已处理数据提供给虚拟输出设备。这样就完成了一个周期</li>
</ol>

<h3 id="toc_16">渲染回调函数</h3>

<p>使用符合<code>AURenderCallback</code>类型的渲染回调函数，将音频数据从磁盘或内存提供到audio unit输入总线，此时当<code>audio unit</code>需要一片数据时 会调用此回调</p>

<p>渲染回调有严格的性能要求，<code>render callbak</code>在实时优先的线程上，后续的render call会异步到达。因此，渲染回调所做工作都是在这种时间受限的环境中进行。如果在下一个渲染调用到达时，回调仍在响应上一个渲染调用生成sample frames，那么声音就会出现空白。因此不能在回调函数中执行锁、分配内存、访问文件系统、网络连接等耗时任务</p>

<h4 id="toc_17">理解渲染回调函数</h4>

<pre><code class="language-objectivec">static OSStatus MyAURenderCallback (
    void                        *inRefCon,
    AudioUnitRenderActionFlags  *ioActionFlags,
    const AudioTimeStamp        *inTimeStamp,
    UInt32                      inBusNumber,
    UInt32                      inNumberFrames,
    AudioBufferList             *ioData
) { /* callback body */ }
</code></pre>

<p><code>inRefCon</code>参数指向您指定的编程上下文。此上下文的目的是为回调函数提供计算给定渲染调用所需的任何音频输入数据或状态信息</p>

<p><code>IoActionFlags</code>参数使回调可以向音频单元提供没有音频要处理的提示。在要为其输出静默的回调调用期间，在回调主体中使用以下语句：<br/>
<code>*ioActionFlags |= kAudioUnitRenderAction_OutputIsSilence;</code><br/>
如果要保持静音，还必须将ioData参数指向的缓冲区显示设置为0</p>

<p><code>InTimeStamp</code>表示调用回调时间。其中<code>mSampleTime</code>字段是一个样本帧计数器。每次调用该回调时，mSampleTime字段的值将按<code>inNumberFrames</code>参数中的数字递增。</p>

<p><code>InBusNumber</code>参数指示调用回调的音频单元总线，允许您根据此值在回调中分支。此外，在将回调函数附加到音频单元时，可以为每个总线指定不同的上下文（inRefCon）</p>

<p><code>InNumberFrames</code>参数指示在当前调用中要求回调提供的音频样本帧的数量。您可以将这些帧提供给ioData参数中的缓冲区。</p>

<p><code>IoData</code>参数指向调用回调时必须填充的音频数据缓冲区。您放入这些缓冲区的音频必须符合调用回调的总线的音频流格式。</p>

<h3 id="toc_18">音频流格式启用数据流</h3>

<h4 id="toc_19">AudioStreamBasicDescription 结构</h4>

<p>在app和音频硬件中传递的音频数据为<code>AudioStreamBasicDescription</code>结构。常简称为ASBD</p>

<pre><code class="language-objectivec">struct AudioStreamBasicDescription {
    Float64 mSampleRate;
    UInt32  mFormatID;
    UInt32  mFormatFlags;
    UInt32  mBytesPerPacket;
    UInt32  mFramesPerPacket;
    UInt32  mBytesPerFrame;
    UInt32  mChannelsPerFrame;
    UInt32  mBitsPerChannel;
    UInt32  mReserved;
};
typedef struct AudioStreamBasicDescription  AudioStreamBasicDescription;

//示例  定一个立体声流信息
size_t bytesPerSample = sizeof (AudioUnitSampleType);
AudioStreamBasicDescription stereoStreamFormat = {0};
 
stereoStreamFormat.mFormatID          = kAudioFormatLinearPCM;
stereoStreamFormat.mFormatFlags       = kAudioFormatFlagsAudioUnitCanonical;
stereoStreamFormat.mBytesPerPacket    = bytesPerSample;
stereoStreamFormat.mBytesPerFrame     = bytesPerSample;
stereoStreamFormat.mFramesPerPacket   = 1;
stereoStreamFormat.mBitsPerChannel    = 8 * bytesPerSample;
stereoStreamFormat.mChannelsPerFrame  = 2;           // 2 indicates stereo
stereoStreamFormat.mSampleRate        = graphSampleRate;
</code></pre>

<ol>
<li>首先，确定采样值数据类型，本示例采用<code>AudioUnitSampleType</code>(在不同平台不同，iOS中代表8.24定点整数)</li>
<li>初始化ASBD</li>
<li>为mFormatID指定kAudioFormatLinearPCM。因为audio unit使用未压缩的音频数据</li>
<li>为大多数unit指定<code>kAudioFormatFlagsAudioUnitCanonical</code>.其负责<code>AudioUnitSampleType</code>类型的线性PCM采样值bit的所有布局细节。<br/>
注意：对于某些unit采用非典型的数据格式，即使用不同的数据类型<code>mFormatFlags</code>字段使用不同的标志集合。因此，使用特定audio unit请使用正确的数据合适和标志格式</li>
<li><code>mBytesPerPacket、mBytesPerFrame、mFramesPerPacket、mBitsPerChannel</code>可以参阅<a href="https://developer.apple.com/documentation/coreaudio/audiostreambasicdescription">AudioStreamBasicDescription</a> 和 示例项目<code>Audio Mixer（MixerHost）</code>中ASBD示例</li>
<li><code>mChannelsPerFrame</code>1为单声道  2为立体声</li>
<li>根据设置的采样率设置<code>mSampleRate</code></li>
</ol>

<p>可以使用<code>CAStreamBasicDescription.h</code>文件中提供的方法，而不必每个字段的指定ASBD。使用<code>SetCanonical和SetCanonical</code>方法</p>

<h4 id="toc_20">了解在何处以及如何设置流格式</h4>

<p>必须在<code>processing graph</code>的关键点设置音频流格式。在其他时候，系统设置格式，在其他地方，<code>audio unit connection</code>将流格式从一个audio unit传递到另一个audio unit</p>

<p>IOS设备上的音频输入和输出硬件具有系统确定的音频流格式。这些格式始终是未压缩的，线性PCM格式，并且是交织的。系统将这些格式强加在音频处理图中的I / O单元的朝外侧面上<br/>
<img src="media/15773512971221/15776914766256.jpg" alt="" style="width:825px;"/></p>

<p>app负责在I/O单元元素的向内侧面建立音频流格式,在app格式和硬件格式间执行必要转换。<br/>
应用程序还负责在图形中其他需要的地方设置流格式，例如<code>Multichannel Mixer</code>单元的输出处,只需要设置格式的一部分，特别是采样率。</p>

<p><code>audio unit connection</code>:将音频数据流格式从其源音频单元的输出传播到其目标音频单元的输入。这是一个关键点，因此需要强调：流格式的传播是通过音频单元连接进行的，并且仅在一个方向上进行-从源音频单元的输出到目标音频单元的输入。</p>

<blockquote>
<p>请尽可能使用硬件使用的采样率。当您这样做时，I / O单元无需执行采样率转换。可以最大程度地减少能源消耗,并可以使音频质量最大化 </p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CoreAudio]]></title>
    <link href="https://acefish.github.io/15772759555662.html"/>
    <updated>2019-12-25T20:12:35+08:00</updated>
    <id>https://acefish.github.io/15772759555662.html</id>
    <content type="html"><![CDATA[
<p>Apple官网<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/Introduction/Introduction.html#//apple_ref/doc/uid/TP40003577-CH1-SW1">Core Audio Overview</a></p>

<p>Core Audio 提供了在iOS和OS X创建的应用中实现音频功能的接口。基于其，我们可以处理音频平台的所有方面。</p>

<p>在iOS中，其包括录制、回放、声音效果、定位、格式转换和文件流解析，以及：</p>

<ul>
<li>在应用中使用内置的均衡器和混频器</li>
<li>自动访问音频输入、输出硬件</li>
<li>提供在接听电话环境中对应用音频方面的管理API</li>
<li>在不影响音频质量的情况下优化和延长电池寿命</li>
</ul>

<p><code>Core Audio</code>是将C和Object-C的编程接口与系统机密结合在一起，形成灵活的编程环境，从而在整个信号链中保持较低的延迟</p>

<h2 id="toc_0">What is Core Audio？</h2>

<p>Core Audio是iOS和OS X中的数字音频基础服务。包含一系列满足应用处理音频需求的框架。</p>

<h3 id="toc_1">iOS中的Core Audio</h3>

<p><img src="media/15772759555662/15773276986484.png" alt=""/></p>

<p>可以再<code>Audio Unit</code>和<code>Audio Toolbox</code>framework中找到Core Audio <code>Applicaiton-level</code>服务</p>

<ul>
<li>使用<code>Audio Queue Service</code>，记录、回放、暂停、循环和同步音频</li>
<li>使用<code>Audio File Service</code>，从磁盘上读写文件，并且执行音频格式转换</li>
<li>使用<code>Audio Unit Service</code>和<code>Audio Processing Graph Service</code>继承在应用中管理Audio Unit</li>
<li>使用<code>System Sound Service</code>播放系统声音和用户界面音效</li>
<li>使用<code>Audio Session Services</code> 允许在iPhone上下文环境中管理音频行为</li>
</ul>

<h3 id="toc_2">Audio Units</h3>

<p><code>Audio Units</code>是处理音频数据的软件插件。在OS X中一个音频单元可以用无无限数量的通道和应用程序<br/>
在iOS中提供了一系列音频单元，并针对移动平台进行优化。可以自己开发<code>Audio Unit</code>在应用程序中使用，此时你必须将自定义的<code>audio unit</code>连接到静态链接到应用中，因此无法被其他应用使用</p>

<p>iOS中提供的<code>audio unit</code>并没有用户界面，他主要用于在应用中提供低延迟音频，请参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW10">Core Audio Plug-ins: Audio Units and Codecs.</a></p>

<h3 id="toc_3">硬件抽象层</h3>

<p><code>Core Audio</code>使用应用抽象层(<code>Hardware Abstraction Layer</code>HAL)为应用程序和硬件交互提供了一致并可预测的接口。HAL还提供了时序信息，来建华同步或调整延时</p>

<p>在大多数情况下 我们都不会直接和HAL交互。Apple提供了特殊的<code>audio unit</code>(在OS中称为AUHAL，在iOS中称为<code>AURemoteIO</code>单元),可以将音频从一个人音频单元传递给硬件。同样，来自硬件的输入也会通过其进行传输，并且可供后续的音频单元使用</p>

<p><code>AUHAL/AURemoteIO</code>还负责在<code>audio unit</code>和硬件间转换音频数据所需的 数据转换和声道映射</p>

<h2 id="toc_4">Core Audio Essentials</h2>

<p>Apple采用了分层、协作、以任务为中心方式设计了<code>Core Audio</code>接口。</p>

<hr/>

<p>简要介绍接口以及如何协同工作</p>

<hr/>

<h3 id="toc_5">分层架构</h3>

<p><img src="media/15772759555662/15773486612019.png" alt="" style="width:300px;"/></p>

<p>在low-level</p>

<ul>
<li>I/O kit 与驱动程序交互</li>
<li>HAL：硬件抽象层，提供了与设备无关、与驱动无关的接口</li>
<li>MIDI: 提供了处理MIDI流和设备的软件抽象</li>
<li>Host Time Services：用于访问计算机时钟<br/>
我们通常无法访问此层，在Ios中，Core Audio提供了更高层次接口来实现实时音频功能</li>
</ul>

<p>在mid-level: 包括了用于 格式转换、从磁盘读写、解析流、以及使用插件的服务</p>

<ul>
<li><code>Audio Converter Service</code>使应用可以使用音频格式转换器</li>
<li><code>Audio File Services</code> 支持从磁盘读写文件</li>
<li><code>Audio Unit Service</code>和<code>Audio Processing Graph Service</code>使应用程序可以使用数字信号处理插件，例如均衡器和混响器</li>
<li><code>Audio File Stream</code>允许构建可解析流应用程序，例如用于播放网络连接流式文件</li>
<li><code>Core Audio Clock Services</code>支持音频和MIDI同步</li>
</ul>

<p>在high-level： 包括了结合较低层次功能的简化的界面</p>

<ul>
<li><code>Audio Queue Services</code>：允许录制、播放、暂停、循环并且同步音频。它会根据需要采用编码器来处理压缩的的音频格式</li>
<li><code>AVAudioPlayer</code>：提供了简单的OC接口 用于播放和循环音频，来实现倒带和快进功能</li>
<li><code>Extended Audio File Service</code></li>
<li><code>Open AL</code>是用于位置音乐的开源Open AL的实现，基于系统提供的<code>3D Mixer audio unit</code>。一般用于游戏开发</li>
</ul>

<h3 id="toc_6">framework</h3>

<ul>
<li><code>AudioToolbox.framework</code> 提供了mid-level和high-level服务接口。在iOS中，框架包括了<code>Audio Session</code>服务，这个session接口是用于充当手机等设备的上下文中官路应用程序的音频行为</li>
<li><code>AudioUnit.framework</code> 使用应用程序可以使用音频插件，包括audio unit和解码器</li>
<li><code>CoreAudio.framework</code>提供了跨<code>core audio</code>使用的的数据类型以及用于低层服务的接口</li>
<li><code>CoreAudioKit.framework</code> 提供一个小的接口用于为音频单元创建用户界面(iOS不可用)</li>
<li><code>CoreMIDI.framework</code>使应用程序可以处理MIDI数据并配置MIDI网络(在iOS中不可用)</li>
<li><code>CoreMIDIServer.framework</code> 使MIDI驱动程序和OS X MIDI服务器通信。(iOS中不可用)</li>
<li><code>OpenAL.framework</code> 提供与OpenAL使用的接口</li>
</ul>

<blockquote>
<p>注意Core Audio framework并不是其它framework的集合，而是一个框架</p>
</blockquote>

<hr/>

<p>Core Audio的设计原理，使用模式和编程习惯</p>

<hr/>

<h3 id="toc_7">代理对象</h3>

<p><code>Core Audio</code>使用代理对象(proxy object)的概念来呈现例如：文件、流、audio player对象等。 </p>

<h3 id="toc_8">属性、范围和元素</h3>

<p>大多数的<code>Core Audio</code>接口都使用属性机制来管理对象状态或者重定义对象行为。属性是键值对</p>

<ul>
<li>属性键通常是带有助记名的常量 例如<code>kAudioFilePropertyFileFormat、kAudioQueueDeviceProperty_NumberChannels</code>等</li>
<li>属性值为适合该数据类型的特定类型 例如 <code>void *、Float64、AudioChannelLayout结构体</code></li>
</ul>

<p><code>Core Audio</code>接口使用访问器函数从对象中检索属性值，并在属性可写时，更改属性值。还可以找到第三种访问器函数，用于获取有关属性的信息。例如：<code>Audio Unit Service</code>函数<code>AudioUnitGetPropertyInfo</code>可以获取给定属性值数据类型的大小以及是否可写、<code>Audio Queue Service</code>中的<code>AudioQueueGetPropertySize</code>可以获取指定属性值大小</p>

<p><code>Core Audio</code>提供了一种机制来通知应用程序 属性值已经更改。请参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW38">Callback Functions: Interacting with Core Audio.</a></p>

<p>在某些情况下，属性会整体应用于音频对象。<br/>
而其他的一些Core Audio对象具有内部结构，每部分都可能有自己的属性。例如，在audio unit有输入范围、输出范围以及全局范围。audio unit的输入输出范围由一个或多个元素组成，当调用<code>AudioUnitGetProperty</code>函数获取<code>kAudioUnitProperty_AudioChannelLayout</code>属性，除了要指定那个unit之外，还要指定范围(input为/output)以及元素(0、1、2等)</p>

<h3 id="toc_9">对Core Audio交互的回调函数</h3>

<p>许多<code>Core Audio</code>使用回调函数和应用程序通信</p>

<ul>
<li>向应用程序交付一组信息的音频数据(例如：用于录制，在回调中将新的数据写入磁盘)</li>
<li>从应用程序请求一组新的音频数据(例如：用于播放，回调从磁盘读取并提供新数据)</li>
<li>告诉应用程序 软件的对象状态已经改变</li>
</ul>

<p>对于回调，操作系统会触发在应用程序中实现的行为。</p>

<ol>
<li>根据模板实现回调函数</li>
<li>将回调函数注册到要交互的对象中</li>
</ol>

<hr/>

<p>Core Audio如何处理文件，流，记录和播放以及插件</p>

<hr/>

<h3 id="toc_10">Audio Data Formats</h3>

<p><code>Core Audio</code> 使无需了解了解音频格式的详细知识。</p>

<blockquote>
<p>音频数据格式本质上描述了音频数据，包括采样率，位深度和包化等内容<br/>
音频文件格式描述了声音文件的音频数据，音频元数据和文件系统元数据如何在磁盘上排列。有的文件格式只能包含一种音频数据格式，而其他文件格式可以包含多种音频数据格式</p>
</blockquote>

<h4 id="toc_11">Core Audio中的通用数据类型</h4>

<p>在Core Audio中可以使用声明在<code>CoreAudioType.h</code>中的结构体<code>AudioStreamBasicDescription</code>和<code>AudioStreamPacketDescription</code>两个通用数据类型来表示任何音频数据格式</p>

<p><code>AudioStreamPacketDescription</code>类型，描述某些压缩的的音频数据格式</p>

<blockquote>
<p>常将<code>audio stream basic description</code>简写为ASBD</p>
</blockquote>

<h4 id="toc_12">获取声音文件数据格式</h4>

<p>我们虽然可以手动设置ASBD的成员值，但是可能某些填写会出现问题。我们可以将结构成员值设置为0，然后使用Core Audio接口填充结构体。例如可以使用<code>Audio File Service</code>为磁盘上文件提供完整的ASBD</p>

<h4 id="toc_13">音频数据格式标准</h4>

<p>平台不同，存在一种或者两种 <code>标准的</code>音频数据格式</p>

<p>在Core Audio中的标准格式常用于：</p>

<ul>
<li>转换的中间格式</li>
<li>优化Core Audio service 的 格式</li>
<li>如果在不指定ASBD，则作为默认或者假定的格式</li>
</ul>

<p>在iOS中的格式标准：</p>

<ul>
<li>iOS中16bit整数样本的输入和输出的线性PCM</li>
<li>iOS的audio unit和其它音频处理 8.24bit定点样本的非交错线性PCM</li>
</ul>

<blockquote>
<p>8.24有时写为Q8.24或fx8.24。定点采样大小用作iOS中处理线性PCM音频的规范音频采样类型，代替32位浮点采样。在8.24音频样本中，小数点左侧有8位，形成该值的整数（或“幅值”）部分，右侧有24位，构成小数部分</p>
</blockquote>

<pre><code class="language-objectivec">//44.1kHZ采样率的双通道 标准iPhone audio unit的格式

struct AudioStreamBasicDescription {
    mSampleRate       = 44100.0;
    mFormatID         = kAudioFormatLinearPCM;
    mFormatFlags      = kAudioFormatFlagsAudioUnitCanonical;
    mBitsPerChannel   = 8 * sizeof (AudioUnitSampleType);                    // 32 bits
    mChannelsPerFrame = 2;
    mBytesPerFrame    = mChannelsPerFrame * sizeof (AudioUnitSampleType);    // 8 bytes
    mFramesPerPacket  = 1;
    mBytesPerPacket   = mFramesPerPacket * mBytesPerFrame;     // 8 bytes
    mReserved         = 0;
};
//这些常量在 CoreAudioTypes.h 头文件声明
</code></pre>

<h4 id="toc_14">Magic Cookies</h4>

<p>在Core Audio领域， <code>magic cookie</code>是连接压缩声音文件或流的一系列不透明元数据<br/>
元数据为解码器提供了解压文件或者流需要的详细信息。<br/>
可以将magic cookie视为黑盒，依靠<code>Core Audio</code>功能去复制、读取、使用包含的元数据</p>

<pre><code class="language-objectivec">- (void) copyMagicCookieToQueue: (AudioQueueRef) queue fromFile: (AudioFileID) file {
 
    UInt32 propertySize = sizeof (UInt32);
 
    OSStatus result = AudioFileGetPropertyInfo (
                            file,
                            kAudioFilePropertyMagicCookieData,
                            &amp;propertySize,
                            NULL
                        );
 
    if (!result &amp;&amp; propertySize) {
 
        char *cookie = (char *) malloc (propertySize);
 
        AudioFileGetProperty (
            file,
            kAudioFilePropertyMagicCookieData,
            &amp;propertySize,
            cookie
        );
 
        AudioQueueSetProperty (
            queue,
            kAudioQueueProperty_MagicCookie,
            cookie,
            propertySize
        );
 
        free (cookie);
    }
}

</code></pre>

<h4 id="toc_15">Audio Data Packets</h4>

<p>数据包是一个或者多个帧的集合，对于给定音频格式，这是最小有意义的帧的集合。因此，它是音频文件中代表时间单位最好的音频数据单元。<br/>
在<code>Core Audio</code>中通过对数据包进行计数来实现同步功能，可以使用data packets来计算有用的音频数据缓冲区大小</p>

<p>在ASBD中用<code>mBytesPerPacket</code>和<code>mFramesPerPacket</code>成员变量描述有关数据包的基本信息，因为某些数据包并不是恒定不变的，因此对于需要其他额外信息，可以使用<code>audio stream packet description</code><br/>
三种packet包类型</p>

<ul>
<li>CBR(contant bit rate)：恒定比特率，所有数据包大小相同</li>
<li>VBR(variable bit rate):可变比特率，所有数据包帧数相同，但是每个采样值的bit可能不同</li>
<li>VFR(variable frame rate):数据包具有变化的帧数。这种类型格式并不常用</li>
</ul>

<p>对于VBR和VFB需要使用<code>AudioStreamPacketDescription</code>结构体进行描述。每个这样的结构体都描述了声音文件中的单个数据包，当需要录制或者播放这样的声音文件，需要这种结构的数组</p>

<p>在<code>Audio File Services 和 Audio File Stream Services</code>中允许使用packet。例如，<code>AudioFile.h</code>文件中的<code>AudioFileReadPackets</code>从磁盘的声音文件中读取一系列的放置在buffer中的packets，同时返回一组<code>AudioStreamPacketDescription</code>结构体描述每个packet</p>

<p>对于常用的CBR和VBR格式中，file或者stream每秒返回包的数量是固定的，包意味着格式的时间单位。在应用程序中计算实际的音频缓冲区大小时，可以使用包。</p>

<h3 id="toc_16">数据格式转换</h3>

<p>要将音频数据从一种格式转换为另一种，可以使用音频转换。<br/>
可以做一些简单的转换，例如：采样率、交错或非交错编码。当然也可以做复杂一点的转换，例如：编解码音频。</p>

<ul>
<li>解码音频格式(例如AAC格式)为PCM格式</li>
<li>将PCM线性数据转换为其他音频格式</li>
<li>在线性PCM格式之间转换，例如，将16位带服务号整数线性PCM转为8.24定点线性PCM</li>
</ul>

<p>当使用<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW32">Audio Queue Service</a>时，将会自动获得合适的转换器。</p>

<h3 id="toc_17">声音文件</h3>

<p>每当想在应用程序中使用声音文件时，可以使用<code>Audio File Services</code>。其为访问文件中包含的音频数据和元数据以及创建声音文件提供了强大的抽象功能</p>

<p><code>Audio File Services</code> 不仅可以使用基础功能如唯一的file ID和数据格式，还可以使用区域和标记、循环播放、播放方向以及SMPTE时间码等</p>

<p><code>Audio File Services</code>还可以用来发现系统特征。使用的功能为<code>AudioFileGetGlobalInfoSize</code>和<code>AudioFileGetGlobalInfo</code>，在<code>AudioFile.h</code>中声明了一列属性可以获取系统特征：</p>

<ul>
<li>可读文件类型</li>
<li>可写文件类型</li>
<li>对于可写类型，可以将以您数据格式放入文件中</li>
</ul>

<h4 id="toc_18">创建一个新的声音文件</h4>

<p>当创建一个声音文件进行记录时：</p>

<ul>
<li>文件的系统路径，以CFURL或者NSURL形式</li>
<li>要创建的文件类型标识，如<code>AudioFile.h</code>中声明的<code>Audio File Types</code>所示</li>
<li>放入文件中的ASBD，在大多数情况下，之粗腰提供部分ASBD，然后要求Audio File Service为你填充接下来部分<br/>
将这三部分作为参数传递给<code>AudioFileCreatWithURL</code>函数，返回一个<code>AudioFileID</code>对象</li>
</ul>

<pre><code class="language-objectivec">AudioFileCreateWithURL (
    audioFileURL,
    kAudioFileCAFType,
    &amp;audioFormat,
    kAudioFileFlags_EraseFile,
    &amp;audioFileID   // the function provides the new file object here
);
</code></pre>

<h4 id="toc_19">打开声音文件</h4>

<p>使用<code>AudioFileOpenUrl</code>函数打开文件，为该函数提供文件的URL、文件类型常量。以及使用文件的访问权限。</p>

<p>然后使用属性标识符号和<code>AudioFileGetPropertyInfo</code>和<code>AudioFileGetProperty</code>函数来检索需要的文件信息，常用的属性有</p>

<ul>
<li><code>kAudioFilePropertyFileFormat</code></li>
<li><code>kAudioFilePropertyDataFormat</code></li>
<li><code>kAudioFilePropertyMagicCookieData</code></li>
<li><code>kAudioFilePropertyChannelLayout</code>
在<code>Audio File Servie</code>中包含许多类似标识,可以快速获取文件中存在的元数据 例如：区域标记、版权信息、播放速度等</li>
</ul>

<p>当VBR文件很长时，获取整个数据包表可能会话费大量时间，此时可以使用<code>kAudioFilePropertyPacketSizeUpperBound</code>和<code>kAudioFilePropertyEstimatedDuration</code>。可以使用它们来快读估算VBR声音文件的持续时间和数据包数量，而不需要解析整个文件来获取确切的数量</p>

<h4 id="toc_20">读写声音文件</h4>

<p>在iOS中  通常使用<code>Audio File Services</code>来读写音频文件，其两者是互为镜像操作。两个操作都会阻塞直到完成，并且都可以使用packet和byte操作，但是除非有特殊要求，我们总是使用packets 原因如下：</p>

<ul>
<li>按包读取和写入是VBR数据的唯一选择</li>
<li>使用基于数据包的操作可以更轻松地计算持续时间</li>
</ul>

<p>iOS中 也可以使用<code>Audio File Stream Service</code>从磁盘读取音频数据。参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW33">Sound Stream</a></p>

<p><code>Audio Queue Service</code>声明在<code>Audio ToolBox</code>的<code>AudioQueue.h</code>中，用于录制和播放Core Audio 可以参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW32">Recording and Playback using Audio Queue Services</a></p>

<h4 id="toc_21">iPhone 支持的音频文件格式</h4>

<p>有关iOS中可用的音频数据格式的信息，请参阅编<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW26">解码器</a><br/>
<img src="media/15772759555662/15774334676700.jpg" alt="" style="width:625px;"/></p>

<h4 id="toc_22">CAF</h4>

<p>CAF是iOS或者OSX 本机音频文件格式(核心音频格式)，可以包含平台支持的任何音频数据格式</p>

<h3 id="toc_23">Sound Streams</h3>

<p>与基于磁盘的声音文件不同，音频文件流无法访问其开始和结束的音频数据。并且流数据也可能是不可靠的，可能存在信号丢失、不连续、暂停的情况，取决于用户的网络状况。</p>

<p>使用<code>Audio File Stream</code>来使应用程序处理流和其复杂情况，并解析。<br/>
使用<code>Audio File Stream</code>需要创建一个<code>AudioFileStreamID</code>类型的对象来作为流的代理对象。也可以通过该对象属性获取当前流信息(例如，当Audio File Stream Services确定bit rate后，会将其设置到该对象的<code>kAudioFileStreamProperty_BitRate</code>属性)。</p>

<p>因为<code>Audio File Stream</code>执行了解析工作，因此应用程序 需要定义两个回调函数来使应应用程序响应给定的音频数据集合和其他信息</p>

<ol>
<li><p>需要为stream object对象的属性改变那定义回调。至少，需要这个回调来为<code>kAudioFileStreamProperty_ReadyToProducePackets</code>属性的改变做出响应。此时，我们通常流程如下：</p>
<ol>
<li>用户点击播放按钮，请求流开始播放</li>
<li><code>Audio File Stream Services</code>开始解析流</li>
<li>当有足够多的音频数据包解析发送到应用程序时，<code>Audio File Stream Services</code>设置streamObject的<code>kAudioFileStreamProperty_ReadyToProducePackets</code>属性为true</li>
<li>触发应用程序的属性改变回调</li>
<li>回调函数采取适当的措施，例如设置音频队列来播放流信息</li>
</ol></li>
<li><p>需要为音频数据设置回调，无论何时<code>Audio File Stream Services</code>获取到一组完整的音频数据包是都会调用这个回调，通常将其发送到<code>Audio Queue Service</code>立即进行播放。可以参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW32">Recording and Playback using Audio Queue Services</a></p></li>
</ol>

<h3 id="toc_24">Audio Session</h3>

<p>在iOS上，当播放音频时 此时手机可能有更重要的事情 例如打电话，我们需要对这些情况做出正确的应对。</p>

<p><code>Audio Session</code>是应用程序和iOS系统中的中介。每个iPhone应用只有一个<code>Session</code>。将其配置为应用程序的播放音频目的。</p>

<p>可以使用<code>AudioToolBox</code>中的<code>AudioService.h</code>声明的接口来配置应用程序。</p>

<table>
<thead>
<tr>
<th>Category</th>
<th>用于标识应用程序的音频行为，将音频意图指示给iOS(例如在锁屏状态是否继续音频)</th>
</tr>
</thead>

<tbody>
<tr>
<td>播放中断和播放路线改变</td>
<td>Audio Session会在音频中断，中断结束，和硬件路由改变(例如插拔耳机)发布通知。这些通知可以优雅响应音频环境更改(例如因为来电而导致的中断)</td>
</tr>
<tr>
<td>硬件特点</td>
<td>可以查询Audio Session，发现运行应用程序的设备特性，例如硬件采样率，硬件声道数，以及音频输入是否可用</td>
</tr>
</tbody>
</table>

<h4 id="toc_25">默认行为</h4>

<p>在audio session中会自带一些默认行为</p>

<ul>
<li>当用户在“响铃/静音”之间切换时， 音频也将会静音</li>
<li>当用户按下“睡眠/唤醒”按钮，或者在“自动锁定”时间到期时， 音频也将会静音</li>
<li>当你的音频开始播放时，设备上其它音频  将会被静音</li>
</ul>

<p>这组行为是有默认的<code>audio session category</code>(<code>kAudioSessionCategory_SoloAmbientSound</code>)指定。iOS中提供了广泛的音频需求 分类,可以在应用程序启动和运行时指定需求分类</p>

<p><code>audio session</code>的默认行为足够进行iPhone的音频开发，但是除了特殊情况，默认行为并不适合于正在传输的应用程序</p>

<h4 id="toc_26">中断：暂停和激活</h4>

<p>在默认行为中明显缺少的一项功能是能够在中断后重新激活自身。<br/>
<code>audio session</code>有两个主要状态：活跃和非活跃状态。而音频仅仅在你的audio session处于活跃状态才会起作用</p>

<p>应用启动后，默认audio session处于活跃状态。但是，加入打来电话，session将立即进入费活跃状态，并且auduo被停止，这即为中断。此时，假如用户选择忽略电话，应用程序将会继续运行，但是audio session处于费活跃状态，音频并不会正常播放</p>

<p>假如在应用中使用OpenAL、I/O audio unit、或者<code>Audio queue Service</code>播放音频，则必须编写<mark>监听中断的回调函数</mark>并且在audio session中断结束时 显示的重新激活。可以参阅<a href="https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">Audio Session Programming Guide</a> 提供的详细信息 和 示例代码<br/>
假如在应用中使用的时 <code>AVAudioPlayer</code>，则该类或为你重新激活audio session</p>

<h4 id="toc_27">确定音频输入是否可用</h4>

<p>对于一个录制的应用，只能在硬件音频输入设备可用时 进行录制。此时可以使用<code>audio session</code>中的<code>kAudioSessionProperty_AudioInputAvailable</code>属性。</p>

<pre><code class="language-objectivec">UInt32 audioInputIsAvailable;
UInt32 propertySize = sizeof (audioInputIsAvailable);
 
AudioSessionGetProperty (
    kAudioSessionProperty_AudioInputAvailable,
    &amp;propertySize,
    &amp;audioInputIsAvailable // A nonzero value on output means that
                           // audio input is available
);
</code></pre>

<h4 id="toc_28">使用Audio Session</h4>

<p>一个应用程序 一次仅仅只有一个音频category。因此，在一段时间 所有音频都会遵循这个活动category(当使用<code>System Sound Service</code>时是例外的，此类音频用户将饱和用户界面音效，始终使用优先级最低的category)。可以查看<a href="https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">Audio Session Programming Guide</a> 中的Responding to Interruptions 描述了所有category</p>

<blockquote>
<p>模拟器不会模拟session  因此请在真机上进行测试</p>
</blockquote>

<h3 id="toc_29">使用AVAudioPlayer 播放</h3>

<p><code>AVAudioPlayer</code>为音频播放提供了简单的OC接口。如果应用并不需要立体声定位和精确同步，或者也不需要播放网络流，可以受用此类</p>

<p>其提供了以下功能：</p>

<ul>
<li>播放任何时间长度的声音</li>
<li>播放磁盘文件或者内存缓存中的声音</li>
<li>循环播放</li>
<li>同时播放多种声音</li>
<li>控制正在播放声音的相对播放级别</li>
<li>在声音文件中寻找特定点进行播放，支持诸如快进快退等功能</li>
<li>获取可用于<code>audio level metering</code>的数据</li>
</ul>

<p><code>AVAudioPlayer</code>可以播放任何iOS中可用的任何音频格式声音。可以参阅<a href="https://developer.apple.com/documentation/avfoundation/avaudioplayer">AVAudioPlayer Class Reference.</a></p>

<p>与<code>I/O audio unit以及OpenAL</code>不同，<code>AVAudioPlayer</code>并不需要使用<code>audio session</code>。其自动会在中断后重新激活，假如想要默认audio session category指定的默认行为，可以将默认的<code>audio session</code>和<code>audio player</code>一起使用</p>

<p>使用<code>AVAudioPlayer</code>需要指定 声音文件，<code>prepare play</code>并指定delegate。可以使用delegate来处理终端，并且在播放完毕后，更新用户界面。可以参阅<a href="https://developer.apple.com/documentation/avfoundation/avaudioplayerdelegate">AVAudioPlayerDelegate Protocol Reference.</a></p>

<h3 id="toc_30">使用Audio Queue Service 播放和录制</h3>

<p><code>Audio Queue Service</code>提供了一种直接、低开销的方式 记录和播放音频。可以使无需访问硬件接口即可使用硬件录制和播放设备。还可以在无需了解编码器工作原理，使用复杂的编解码器。</p>

<p>尽管其为<code>high-level</code>接口，但是也支持一些高级功能，例如：提供了细化的定时控制，支持计划的播放和同步，可以使用其来同步多个音频队列，播放声音，并且让独立控制对个声音的播放音量执行loop。<code>Audio Queue Service</code>和<code>AVAudioPlayer</code>是在iPhone上播放压缩的音频的仅有的方式</p>

<p><strong>我们通常会将 <code>Audio Queue Servie</code>和<code>Audio File Service</code>或者<code>Audio File Stream</code>结合使用</strong></p>

<h4 id="toc_31">Audio Queue 播放和录制功能的回调函数</h4>

<p>对于录音，实现一个回调函数，该函数接收<code>audio queue</code>对象提供的<code>audio data buffer</code>并且并将其存入磁盘，当要有新的录音数据buffer传入时，<code>audio queue</code>对象将会调用回调<br/>
<img src="media/15772759555662/15775129719772.png" alt="" style="width:400px;"/><br/>
对于播放则和录制刚好相反。当audio queue对象，需要另外buffer才能播放时，就会调用该函数。然后，回调会从磁盘读取给定数量的<code>audio packet</code>，并将其交给audio queue object buffer。此时<code>audio queue object buffer</code>将会一次播放该buffer</p>

<p><img src="media/15772759555662/15775145378401.png" alt="" style="width:400px;"/></p>

<h4 id="toc_32">创建queue对象</h4>

<p>首先创建一个 audio queue 对象，尽管有两种类型，但类型均为<code>AudioQueueRef</code></p>

<ul>
<li>录制对象 <code>AudioQueueNewInput</code>函数</li>
<li>播放对象 <code>AudioQueueNewOutput</code>函数</li>
</ul>

<p>当创建一个audio queue对象来播放时，需要以下三步</p>

<ol>
<li>创建一个数据结构来管理audio queue所需的信息，例如：要播放的数据的音频格式</li>
<li>定义一个用于管理audio queue buffer的回调函数。回调使用<code>Audio File Service</code>来读取要播放的文件</li>
<li><p>使用<code>AudioQueueNewOutput</code>实例化<code>audio queue</code></p>
<pre><code class="language-objectivec">static const int kNumberBuffers = 3;
// Create a data structure to manage information needed by the audio queue<br/>
struct myAQStruct {<br/>
AudioFileID                     mAudioFile;<br/>
CAStreamBasicDescription        mDataFormat;<br/>
AudioQueueRef                   mQueue;<br/>
AudioQueueBufferRef             mBuffers[kNumberBuffers];<br/>
SInt64                          mCurrentPacket;<br/>
UInt32                          mNumPacketsToRead;<br/>
AudioStreamPacketDescription    *mPacketDescs;<br/>
bool                            mDone;<br/>
};<br/>
// Define a playback audio queue callback function<br/>
static void AQTestBufferCallback(<br/>
void                   *inUserData,<br/>
AudioQueueRef          inAQ,<br/>
AudioQueueBufferRef    inCompleteAQBuffer<br/>
) {<br/>
myAQStruct *myInfo = (myAQStruct *)inUserData;<br/>
if (myInfo-&gt;mDone) return;<br/>
UInt32 numBytes;<br/>
UInt32 nPackets = myInfo-&gt;mNumPacketsToRead;<br/>
AudioFileReadPackets (<br/>
    myInfo-&gt;mAudioFile,<br/>
    false,<br/>
    &amp;numBytes,<br/>
    myInfo-&gt;mPacketDescs,<br/>
    myInfo-&gt;mCurrentPacket,<br/>
    &amp;nPackets,<br/>
    inCompleteAQBuffer-&gt;mAudioData<br/>
);<br/>
if (nPackets &gt; 0) {<br/>
    inCompleteAQBuffer-&gt;mAudioDataByteSize = numBytes;<br/>
    AudioQueueEnqueueBuffer (<br/>
        inAQ,<br/>
        inCompleteAQBuffer,<br/>
        (myInfo-&gt;mPacketDescs ? nPackets : 0),<br/>
        myInfo-&gt;mPacketDescs<br/>
    );<br/>
    myInfo-&gt;mCurrentPacket += nPackets;<br/>
} else {<br/>
    AudioQueueStop (<br/>
        myInfo-&gt;mQueue,<br/>
        false<br/>
    );<br/>
    myInfo-&gt;mDone = true;<br/>
}<br/>
}<br/>
// Instantiate an audio queue object<br/>
AudioQueueNewOutput (<br/>
&amp;myInfo.mDataFormat,<br/>
AQTestBufferCallback,<br/>
&amp;myInfo,<br/>
CFRunLoopGetCurrent(),<br/>
kCFRunLoopCommonModes,<br/>
0,<br/>
&amp;myInfo.mQueue<br/>
);
</code></pre></li>
</ol>

<h4 id="toc_33">控制audio queue播放音量</h4>

<p>有两种方式来控制：</p>

<ol>
<li>直接设置：使用<code>AudioQueueSetParameter</code>函数和<code>kAudioQueueParam_Volume</code>，更改后立即生效</li>
<li>也可以使用<code>AudioQueueEnqueueBufferWithParameters</code>函数设置音频队列缓冲区的播放音量。这些更改在缓存区开始播放时生效</li>
</ol>

<h4 id="toc_34">Indicating Audio Queue Playback Level</h4>

<p>可以通过查询音频对象的<code>kAudioQueueProperty_CurrentLevelMeterDB</code>属性来从音频队列对象获取当前播放级别。此属性的值是<code>AudioQueueLevelMeterState</code>结构的数组，每个通道一个。</p>

<h4 id="toc_35">同时播放多个声音</h4>

<p>要同时播放多个声音，需要为每个声音设置一个audio queue obeject。使用<code>AudioQueueEnqueueBufferWithParameters</code>函数对每个音频队列，将音频的第一个缓冲区设为同时开始</p>

<p>在iPhone上同时播放，音频格式很重要。因为在iOS中某些压缩格式的播放使用了硬件解码器。而一次只能在一下设备上播放以下格式之一的单个实例：</p>

<ul>
<li>AAC</li>
<li>ALAC</li>
<li>MP3<br/>
结余参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40005343">Audio Queue Services Programming Guide</a></li>
</ul>

<h4 id="toc_36">使用OpenAL进行定位播放</h4>

<h3 id="toc_37">系统声音:警报和音效</h3>

<p>当要在不需要设置level、positon、<code>audio session</code>时，播放短声音文件可以使用<code>System Sound Services</code>.其声明在<code>AudioToolBox</code>的<code>AudioServices.h</code>头文件中。在iOS中播放声音时长 不能超过30s</p>

<p>调用<code>AudioServicesPlaySystemSound</code>函数播放指定声效文件。另外，可以在需要提醒用户时，调用<code>AudioServicesPlayAlertSound</code>。当用户设置了静音开关，这两个功能都会引起震动。</p>

<p>也可以在调用<code>AudioServicesPlaySystemSound</code>函数时 使用<code>kSystemSoundID_Vibrate</code>显示调用震动</p>

<p>使用<code>AudioServicesPlaySystemSound</code>需要首先创建sound ID 对象，将声音效果文件注册为系统声音，然后可以播放声音。如果在需要偶尔或者重复播放声音时，请保留声音对象，直到应用退出，如果只使用一次声音的话，可以再播放声音后，在声音完成回调中立即销毁对象，释放内存</p>

<h3 id="toc_38">核心音频插件：Audio unit和解码器</h3>

<p>Core Audio有用于处理音频数据的插件机制。<br/>
在iOS中，系统提供了这些插件，每个应用程序可以使用多个audio unit或者codec</p>

<h4 id="toc_39">Audio Units</h4>

<p>在iOS中<code>Audio Unit</code>为应用提供了实现低延迟的输入和输出机制，以及某些DSP功能，与OSX不同，audio unit并没有自己的使用视图。</p>

<p>可以再iOS和OSX平台上，都可以在Audio Unit framework的<code>AUCompoment.h</code>头文件找到内置音频单元的程序名称</p>

<p>iOS Audio unit 使用8.24bit 定点线性PCM音频数据来进行输入和输出。(converter unit是一个例外)。<code>iOS audio unit</code>为：</p>

<ul>
<li>3D mixer unit： 允许任意数量的单声道输入，每一个都可以为8bit或者18bit的线性PCM。提供一个8.24bit定点PCM的立体声输出。<code>3D mixer unit</code> 在输入上执行采样率转换，并对每个输入通道提供很多控制。这些控制包括声音，静音、panning、距离衰减和这些更改的rate控制。这就是<code>kAudioUnitSubType_AU3DMixerEmbedded</code>unit</li>
<li><code>Multichannel mixer unit</code>:允许任意数量的单声道或者立体声输入，每个输入都可以为16位线性定点PCM。提供一个8.24bit定点PCM的立体声输出。应用可以控制每个通道的静音或非静音，并控制器音量。使用<code>kAudioUnitSubType_MultiChannelMixer</code>单元</li>
<li><code>Convert unit</code>:提供了采样率、位深以及位格式(线性到定点)的转换。iPhone convert unit的标准格式为8.24定点线性PCM。将转换为该格式 或者从该格式转换。使用<code>kAudioUnitSubType_AUConverter</code> unit</li>
<li><code>I/O unit</code>:提供了实时的音频输入和输出，并根据俄需要执行采样率的转换。使用<code>kAudioUnitSubType_RemoteIO</code>unit</li>
<li><code>iPod EQ unit</code>：提供了可在应用中使用的简单均衡器，并有iPhone内置应用的相同预设。使用8.24定点线性PCM作为输入和输出。使用<code>kAudioUnitSubType_AUiPodEQ</code> unit</li>
</ul>

<p>可以参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/AudioUnitProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40003278">Audio Unit Programming Guide</a></p>

<p>在iOS中，应用程序使用<code>audio unit service</code>来发现和加在audio unit。每个audio unit呦类型、子类型和制造上代码3元素组合的唯一标识。其中，类型表示设备的通用用途，子类型更加精确的描述音频单元的功能，在编程上来说并不重要，但是当你的公司提供了多个效果unit，那么每个effct unit都必须有一个单独的子类型来与其他效果器分开。制造商代码则将你表示为unit的开发商。</p>

<p><code>audio unit</code>使用属性来描述其功能和配置信息。<br/>
<code>audio unit</code>使用参数机制进行设置。通常由于用户实时调整,并不是函数参数，而是支持用户调整音频单元行为的机制。</p>

<h4 id="toc_40">解码器</h4>

<p>使用iOS支持的解码器来录制和比方可以平衡音频质量、应用开发灵活性、硬件特性和电池寿命。</p>

<p>iOS中有两组播放编码器。</p>

<ol>
<li>包括可以不受限制使用的高效格式(可以同时播放每种格式的多个示例)
<img src="media/15772759555662/15775540518573.jpg" alt="" style="width:402px;"/></li>
<li>第二组共享一个硬件路径，因此只能一次播放一个
<img src="media/15772759555662/15775541069889.jpg" alt="" style="width:125px;"/></li>
</ol>

<p>iOS支持的录制编码器，并不包括AAC和MP3，因为其CPU高的消耗带来的高耗电<br/>
<img src="media/15772759555662/15775541938605.jpg" alt="" style="width:367px;"/></p>

<h4 id="toc_41">AUGraph</h4>

<p><code>audio processing graph</code>(也可称为<code>AUGraph</code>)定义了一组串在一起执行复杂任务的audio unit 集合。当定义<code>graph</code>时，可以有一个可重用的处理模块，可以在应用程序的信号链中添加和删除模块</p>

<p><code>processing graph</code>通常由<code>I/O unit</code>结束。<code>I/O unit</code>通常通过<code>low-level service</code>和硬件项链，但是这不是必须的，I/O unit可以将其输出发送回应用程序</p>

<p>也可以看到<code>processing graph</code>头结点(node)为I/O单元。<code>I/O unit</code>是唯一可以开始和停止graph中数据流的单元。每个audio unit 都会向其后继者注册一个<code>render callback</code>，允许其后继者向其请求数据，当I/O单元开始数据流时，render方法就会调用其audio unit请求数据，而又会调用其前者的audio unit，以此类推</p>

<p>iOS中只有单一的I/O unit，但是OS X上就多了。</p>

<p><img src="media/15772759555662/15775552612399.jpg" alt="" style="width:391px;"/><br/>
graph中每个audio unit都可以称为node。通过将一个node的输出附加到拎一个node<br/>
的输入来建立连接。</p>

<p>可以将<code>audio processing graph</code>组合为一个更大的graph，其中子图在更大图中也额为一个node<br/>
<img src="media/15772759555662/15775555266441.jpg" alt="" style="width:400px;"/></p>

<blockquote>
<p>每个graph必须以I/O单元结尾。如果其输出只供应用程序使用，应该使用不与硬件连接的<code>generic I/O unit</code>结尾</p>
</blockquote>

<h2 id="toc_42">Core Audio Frameworks</h2>

<p>参阅<a href="https://developer.apple.com/library/archive/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioFrameworks/CoreAudioFrameworks.html#//apple_ref/doc/uid/TP40003577-CH9-SW1">Core Audio Frameworks</a>来查看core audio中的框架以及其核心头文件</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AST]]></title>
    <link href="https://acefish.github.io/15771995229810.html"/>
    <updated>2019-12-24T22:58:42+08:00</updated>
    <id>https://acefish.github.io/15771995229810.html</id>
    <content type="html"><![CDATA[
<p>AST(Abstract syntax tree 抽象语法树) 参见 <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">wiki:Abstract syntax tree</a></p>

<h2 id="toc_0">概念</h2>

<p>抽象语法树是源代码语法结构的抽象表示，以树状形式表现编程语言语法结构。 之所以说是抽象的，因为这里的语法并不会表示真实语法中的每个细节。</p>

<p>与其想对应的为具体语法树(分析树)。一般再源代码翻译和编译过程中，语法分析器创建分析树，然后从分析树生成AST。</p>

<p>抽象语法树还可用于程序分析分析和程序转换系统</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[暗黑模式适配]]></title>
    <link href="https://acefish.github.io/15771854107525.html"/>
    <updated>2019-12-24T19:03:30+08:00</updated>
    <id>https://acefish.github.io/15771854107525.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[App启动优化]]></title>
    <link href="https://acefish.github.io/15771850071304.html"/>
    <updated>2019-12-24T18:56:47+08:00</updated>
    <id>https://acefish.github.io/15771850071304.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[isEqual && Hash]]></title>
    <link href="https://acefish.github.io/15744060179031.html"/>
    <updated>2019-11-22T15:00:17+08:00</updated>
    <id>https://acefish.github.io/15744060179031.html</id>
    <content type="html"><![CDATA[
<p>在iOS系统中，API提供了自动过滤重复元素的容器<code>NSMutableSet/NSSet</code>。但是对于自定义类，必须要同时实现<code>-(bool)isEqual:</code>和<code>-(NSUInteger)hash</code>两个方法。两者的基本关系:<br/>
<strong>两个相等的实例，其hash值一定相等，但是hash值相等的实例，其不一定相等</strong></p>

<h2 id="toc_0">isEqual和 ==</h2>

<p>对于基本类型 == 比较的是值 对于对象 == 比较的是对象地址，对于没有重写isEqual方法的 和 == 没有区别</p>

<p>对于<code>Cocoa Framework</code>中的类型，默认已经有实现好的<code>isEqual</code>方法</p>

<h2 id="toc_1">hash方法</h2>

<p>hash方法主要用于在hashTable 中查询成员用的<br/>
<mark>hash方法在对象被添加到<code>NSSet</code>中 以及 设置对象为<code>NSDictionary</code>的<code>key</code>时被调用</mark><br/>
这个hash值就是用来查找成员中的位置的</p>

<blockquote>
<p>为什么数组中没有用到hash值： 因为数组可能存在重复 就会在出现多个相同hash值 查找效率就没那么高了</p>
</blockquote>

<h2 id="toc_2">hash方法和isEqual方法关系</h2>

<p>在基于hash的<code>NSSet</code>和<code>NSDictionary</code>判断成员是否相等时 会执行一下步骤</p>

<ol>
<li>集合成员hash值是否和目标的hash值相等 如果相等进行step2  不等直接判断不等</li>
<li>hash值相同 的情况下 在进行对象判断 作为判断结果</li>
</ol>

<p>此时 我们应该保证hash方法和isEqual方法的一致性</p>

<p><code>[super hash]</code>是系统默认实现，其返回值和实例所在内存地址值完全一致（注意十六进制和十进制转换后相等）。</p>

<pre><code class="language-objectivec">//hash方法实现
- (NSUInteger)hash {
    return [super hash];
}

//
Person *person1 = [Person personWithName:kName1 birthday:self.date1];
Person *person2 = [Person personWithName:kName1 birthday:self.date1];
NSLog(@&quot;[person1 isEqual:person2] = %@&quot;, [person1 isEqual:person2] ? @&quot;YES&quot; : @&quot;NO&quot;); //yes

NSMutableSet *set = [NSMutableSet set];
[set addObject:person1];
[set addObject:person2];
NSLog(@&quot;set count = %ld&quot;, set.count); //此时有两个元素 就不对了 我们期望是一个元素
</code></pre>

<p>此时为了保证对一致性 以及对hash值的唯一性<br/>
<mark>可以对关键属性的hash值进行位或运算</mark></p>

<pre><code class="language-objectivec">- (NSUInteger)hash {
    return [self.name hash] ^ [self.birthday hash];
}
</code></pre>

<blockquote>
<p>因此，必须同时实现<code>- (BOOL)isEqual:</code>和<code>- (NSUInteger)hash</code>两个方法</p>
</blockquote>

<h2 id="toc_3">示例</h2>

<pre><code class="language-objectivec">
@implementation Person

- (id)copyWithZone:(nullable NSZone *)zone {
    return self;
}

- (BOOL)isEqual:(id)object {
    return NO;
}

- (NSUInteger)hash {
    NSLog(@&quot;执行对象%p的hash值%ld&quot;, self, self.age);
    return self.age;
}

@end

 Person *person1 = [[Person alloc] init];
person1.age = 11;
    
Person *person2 = [[Person alloc] init];
person2.age = 12;
    
Person *person3 = [[Person alloc] init];
person3.age = 12;
    
NSMutableArray *array = [NSMutableArray array];
[array addObject:person1];
[array addObject:person2];
    
if ([array containsObject:person1]) {
    NSLog(@&quot;查询到person1&quot;); //查询到person1  直接==判断
}
if ([array containsObject:person3]) {
    NSLog(@&quot;查询到person3&quot;); //  直接判断isEqual 而不判断hash值
}
    
NSMutableSet *set = [NSMutableSet set];
[set addObject:person1]; //执行对象0x600001690210的hash值11
NSLog(@&quot;person1添加到set中&quot;);
if ([set containsObject:person1]) { //执行对象0x600001690210的hash值11
    NSLog(@&quot;查询到set中的person1&quot;);
}
    
    
NSMutableDictionary *dict = [NSMutableDictionary dictionary];
dict[person1] = @&quot;这是1&quot;; //执行对象0x600001690210的hash值11
dict[person2] = @&quot;这是2&quot;; //执行对象0x600001690220的hash值12
NSLog(@&quot;查询字典中的值&quot;);
//执行对象0x600001690210的hash值11
NSLog(@&quot;找到值  %@&quot;, [dict objectForKey:person1]); //找到值  这是1
//执行对象0x6000010d4200的hash值12
NSLog(@&quot;找到值  %@&quot;, [dict objectForKey:person3]); //找到值  (null)  需要先判断hash值 相等 才会决定是否判断isequal
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UITest]]></title>
    <link href="https://acefish.github.io/15738009699255.html"/>
    <updated>2019-11-15T14:56:09+08:00</updated>
    <id>https://acefish.github.io/15738009699255.html</id>
    <content type="html"><![CDATA[
<p>学习资料<br/>
<a href="https://developer.apple.com/videos/play/wwdc2015/406/">WWDC 2015 Introduction to UI Tests</a></p>

<p><a href="http://www.mokacoding.com/blog/xcode-7-ui-testing/">A first look into UI Tests</a></p>

<p><a href="http://masilotti.com/ui-testing-xcode-7/">UI Testing in Xcode 7</a></p>

<p><a href="https://github.com/ConfusedVorlon/HSTestingBackchannel">HSTestingBackchannel : ‘Cheat’ by communicating directly with your app</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WWDC视频]]></title>
    <link href="https://acefish.github.io/15737184394691.html"/>
    <updated>2019-11-14T16:00:39+08:00</updated>
    <id>https://acefish.github.io/15737184394691.html</id>
    <content type="html"><![CDATA[
<p><a href="https://developer.apple.com/videos/wwdc2019/">WWDC视频</a></p>

<p><a href="https://developer.apple.com/documentation">官方开发文档</a></p>

<p><a href="https://developer.apple.com/library/archive/navigation/">archive官方文档</a></p>

<p><a href="https://developer.apple.com/library/archive/documentation/">查找官方文档</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Xcode11 调试]]></title>
    <link href="https://acefish.github.io/15737182379260.html"/>
    <updated>2019-11-14T15:57:17+08:00</updated>
    <id>https://acefish.github.io/15737182379260.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thread Sanitizer]]></title>
    <link href="https://acefish.github.io/15737170765125.html"/>
    <updated>2019-11-14T15:37:56+08:00</updated>
    <id>https://acefish.github.io/15737170765125.html</id>
    <content type="html"><![CDATA[
<p><a href="http://mrpeak.cn/blog/thread-sanitizer/">如何使用Thread Sanitizer</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[调试学习]]></title>
    <link href="https://acefish.github.io/15737169032344.html"/>
    <updated>2019-11-14T15:35:03+08:00</updated>
    <id>https://acefish.github.io/15737169032344.html</id>
    <content type="html"><![CDATA[
<p><a href="https://devma.cn/blog/2016/11/10/ios-beng-kui-crash-jie-xi/">崩溃Crash解析</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SDWebImage源码分析]]></title>
    <link href="https://acefish.github.io/15732813859540.html"/>
    <updated>2019-11-09T14:36:25+08:00</updated>
    <id>https://acefish.github.io/15732813859540.html</id>
    <content type="html"><![CDATA[
<p>学习自<a href="https://github.com/halfrost/Analyze/blob/master/contents/SDWebImage/iOS%20%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%20---%20SDWebImage.md">SDWebImage源码分析</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[fishhook]]></title>
    <link href="https://acefish.github.io/15731958399871.html"/>
    <updated>2019-11-08T14:50:39+08:00</updated>
    <id>https://acefish.github.io/15731958399871.html</id>
    <content type="html"><![CDATA[
<p>学习自<a href="https://github.com/halfrost/Analyze/blob/master/contents/fishhook/%E5%8A%A8%E6%80%81%E4%BF%AE%E6%94%B9%20C%20%E8%AF%AD%E8%A8%80%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9E%E7%8E%B0.md">动态修改 C 语言函数的实现</a></p>

<p>fishhook是facebook开源的第三方库，主要用于动态修改C语言函数的实现，在框架内部其实只是</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[]]></title>
    <link href="https://acefish.github.io/15724227215760.html"/>
    <updated>2019-10-30T16:05:21+08:00</updated>
    <id>https://acefish.github.io/15724227215760.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
</feed>
